\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[left=35mm, right=35mm, top=35mm, bottom=15mm, includefoot]{geometry}
\usepackage[sorting=none]{biblatex}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{times}
\usepackage[numbib]{tocbibind}  
\usepackage[center]{caption}
\usepackage{adjustbox}
\usepackage{afterpage}
\usepackage{indentfirst}
\usepackage{listings}
\usepackage{seqsplit}
\newtheorem{definition}{Definition}[section]
\addbibresource{bib.bib}
\graphicspath{ {./figures/} }

\title{Comparison of network complexity measures}
\author{Yipei Zhao}

\date{October 2021}
\begin{document}
\begin{titlepage}
    \begin{center}
        \vspace*{1cm}
 
        {\Huge Comparison of network complexity measures}
 
        \vspace{0.5cm}
         
             
        \vspace{1.5cm}
 
        \textbf{Yipei Zhao}\\
        Student number: 170145594\\
        A thesis presented for the degree of\\
        Master of Science Data Analytics\\
        {\large Supervised by Jens Christian Cluassen}\\
        October 2021
 
        \vfill
             

    \end{center}
 \end{titlepage}

\tableofcontents
\pagebreak
\renewcommand{\familydefault}{}
\section{Introduction}
\par
In the modern world, there are many fields where a network can be investigated and studied. For example, electrics in a region would form a large network.\cite{5275353} Also, analysing social networks and use their unique properties to deliver promotion precisely would help firms to increase profits.\cite{stefko2011virtual} In addition, food webs are classic networks that have been studied for many years.\cite{dunne2002food} There are many aspects of a network, but it is vital to ask: what is the complexity of a network? How to design a simple/complex network? The goal of this project is to summarise and compare different complexity measures and demonstrate their properties and uniqueness, they are implemented and used on real networks.
\par
In this project, we want to be specific on transport networks. A transport network transfers either passengers or goods, from an initial location to their destination. The designers have to examine the complexities of the designed networks, to ensure they meet the requirement of the demand. If they don't, how can the networks be modified to reach their corresponding demand? Transport networks are different to other networks in the real world, but before investigating the uniqueness of transport networks, it is vital to study network science in general.\\
\begin{figure}[!h]
    \centering
    \includegraphics[width=\textwidth]{key-bus-routes-in-central-london.pdf}
    \caption{Key routes in central London.\cite{london_bus}}
\end{figure}

\subsection{Graph theory}
Firstly, what do network scientists study? A network contains $n$ nodes/vertices and $m$ edges/links; nodes can be connected using edges. Nodes and edges allow a relationship to be constructed between assets. In a transport network, a node can be considered as a stop on the road, edges are the roads/path connecting stops.
\begin{center}
    "we view network science as the study of the collection,
    management, analysis, interpretation, and presentation of relational data" \cite{brandes_robins_mccranie_wasserman_2013}
\end{center}

\subsubsection{Graph}
\label{graph}
To stay simple, all networks in this project have the following properties:
\begin{itemize}
    \item Undirected. Edges are not allowed to have direction. If the graph is directed, it will be converted into an undirected graph.
    \item All edges are unweighted. All the edges have no weights to be assigned.
    \item No multi edges are allowed. There will be at most one edge between two nodes.
    \item No loops are allowed. Nodes are not allowed to connect to themselves. 
\end{itemize}
\begin{definition}{(Graph/network)}
    A graph or network $G$ can be represented by three sets. A node set $V:\{V_1,V_2,V_3...V_n\}$ and an edge set $E:\{E_1,E_2,E_3...E_m\}$ and a function set $\iota$ such that $E\rightarrow V_\alpha\times V_\beta$ implies there is a connection between node $V_\alpha$ and node $V_\beta$.
\end{definition}
\par
In non-mathematical words, a network or a graph is a collection of nodes and edges, each node can connect to other nodes via edges. The border between the term network and graph is very ambiguous, and network scientists use them interchangeably. There are many different types of graphs. A path graph contains $n$ nodes and all nodes are connected in a sequence. A clique has all its nodes connect to all the other nodes. Since a clique is fully connected, this leads to a total number of $n(n-1)/2$ edges.
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.8\textwidth]{path_clique.eps}
    \caption{From left to right: A path with 10 nodes($n=10$) and 9 edges($m=9$); a clique with 10 nodes and 45 edges($m=45$).}
    \label{fig:path_clique}
\end{figure}
\begin{definition}{(Subgraph)}
    A subgraph $G'$ is a slice of a graph $G$, such that $V' \epsilon V$, $E' \epsilon E$ and $\iota'$ matches two nodes belongs to $V'$, i.e. $E'\rightarrow V'_\alpha\times V'_\beta $. All the edges must have their starting and ending points within the subgraph.
\end{definition}
\begin{definition}{(Neighbours)}
    If node $i$ is connected to node $j$, node $i$ is a neighbour of node $j$, vice versa. If node $i$ is connected to node $j,k,l$, the node set $\{j,k,l\}$ is said to be the neighbourhood of node $i.$
\end{definition}
\begin{definition}{(Distance)}
    Distance $d_{i,j}$ in a undirected, unweighted graph is the minimum number of edges required to connect node $i$ and $j$. Average distance $\langle d\rangle$ is defined as $\langle d \rangle = \frac{2}{n(n-1)}\sum_{i,j}d_{i,j}$.
\end{definition}
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.8\textwidth]{simplenetwork.png}
    \caption{A simple network with 6 nodes and 8 edges.}
    \label{fig:simple_network}
\end{figure}
\par
Referring to figure \ref{fig:simple_network}, distance between node 1 and 6 is 3, denoted by $d_{1,6}=d_{6,1}=3$. Distance between node $i$ and node $j$ is equal to node $j$ to node $i$: $d_{i,j}=d_{j,i}$. The average distance of the network is 1.53.

\par
A node $i$ has degree $k_i$ if it has $k$ edges connected to other nodes. For figure \ref{fig:simple_network}, we have $k_1=2,k_2=3,k_3=3,k_4=2,k_5=4,k_6=2$. Thus, the total number of edges $L$ in a network can be calculated:
\begin{equation}
    L = \frac{1}{2}\sum^{i}_{i=0}{k_i}
    \label{eqn:numberoflinks}
\end{equation}
Using equation \ref{eqn:numberoflinks}, we can calculate the total number of links for figure \ref{fig:simple_network} is equal to:\\
\begin{equation}
    L =\frac{1}{2}(2+3+3+2+4+2)=8
\end{equation}
An important parameter of a network is generated using degrees, which is average degree of a network:
\begin{equation}
    \bar{k}=\frac{1}{N} \sum^{i}_{i=0}{k_i}
    \label{eqn:avg_degree}
\end{equation}
or, simply:
\begin{equation}
    \bar{k}=\frac{2L}{N}
    \label{eqn:avg_degree_1}
\end{equation}
Applying formula \ref{eqn:avg_degree} to figure \ref{fig:simple_network}, the average degree is:
\begin{equation}
    \bar{k} = \frac{1}{6}(2+3+3+2+4+2)=\frac{8}{3}
\end{equation}
or equivalently, using formula \ref{eqn:avg_degree_1}:
\begin{equation}
    \bar{k} = \frac{1}{6}(8*2)=\frac{8}{3}
\end{equation}
\begin{definition}{(Clustering coefficient)}
For a given node $i$ with degree $k$, the clustering coefficient is defined as:\\
    $C_i=\frac{2L_i}{k_i(k_i-1)}$\\
    where $L_i$ represents the number of links exists between node $i$'s neighbours.\\
    Clustering coefficient $C_i$ illustrates the proportion of edges exists between a node $i$'s neighbours. 
\end{definition}
\par 
In figure \ref{fig:simple_network}, node 2 and node 4 are node 1's neighbours. There are no edges between node 2 and node 4, so $C_1 = 0$. On the other hand, node 3 has neighbours 2,5 and 6. There are 2 edges between node 2,5 and 6, $C_i=\frac{2*2}{3*2}=\frac{2}{3}$. Average clustering tends to suggest the average connectivity of the network.

\begin{definition}{(Laplacian Matrix)}
    A Laplacian matrix is defined as:
    \begin{equation}
        L_{i,j} = \begin{cases}
            k_{i}& \text{if i=j}\\
            -1 & \text{if an edge exists between node i and j, and i is not equal to j}\\
            0 & \text{otherwise}
        \end{cases}
    \end{equation}
\end{definition}
\par
Diagonal elements $L_{i,i}$ suggests the degree of node $i$.A non-zero non-diagonal element $L_{i,j}$ implies a connected edge between node $i$ and $j$. Laplacian matrix allows quick calculation of the graph properties and a visualisation of the network.

\par
To evaluate different complexity measures, we need to introduce some graph models that are often used to model real world problems.
\subsection{Random graphs}
There are infinite numbers of real networks in the real world, but defining or observing all of them is not feasible. For simulations and comparisons, network scientists introduced the idea of random networks. They are also known as Erd\H{o}s-R\'{e}nyi network in honour of two mathematicians: Paul Erd\H{o}s and Alfr\'{e}d R\'{e}nyi. They have important contributions to understand the properties of a random network\cite{renyi1959random}.\\
\par
There are two definitions of a random network:
\begin{itemize}
    \item $G(n,p)$ network. A network with $n$ nodes will be initialised, there will be at most $(n)(n-1)/2$ edges. Each edge will be instantiated with probability $p$. This approach brings a randomness property to the graph; number of edges $m$. The expectation of $m$ is equal to $p(n)(n-1)/2$.
    \item $G(n,m)$ or $G(n,L)$ network. A network with $n$ nodes will be initialised, $m$ or $L$ edges will be connected from a random node to another random node. Due to the non-randomness of $G(n,m)$ networks, they are used to simulate the behaviour of a random network in this thesis. They will be referred to a random network/random graph in this thesis unless stated otherwise.
\end{itemize}

\par
Previously, the definition of clustering coefficient and average distance were introduced. For a given random graph, the clustering coefficient and average distance can be calculated using formulas.\cite{barabási2016network} The average clustering coefficient of a random graph is $p$, or $2m/((n)(n-1))$(number of instantiated edges divided by total number of possible edges). The clustering coefficient is used to illustrate the ratio between connected links and possible links between a node's neighbours. If there are $k$ neighbours of a node, there can be at most $k(k-1)/2$ edges between the neighbours. In these $k(k-1)/2$ edges, only $p$ of them will be instantiated. Thus, the ratio of connected edges and possible edges becomes $\frac{pk(k-1)/2}{k(k-1)/2}=p$. Additionally, average distance of a random graph $L_r \approx \frac{ln(n)}{ln\bar{(k)}}\approx \frac{ln(n)}{ln(2m/n)}$. To be noticed, both parameters are expectations.

\subsection{Rewiring}
\label{rewiring}
Network scientists desire more techniques to allow them to modify the properties and variables of a network. Network scientists would use a technique named rewiring to change the properties and parameters of a network, and monitor the change of parameters with respect to the rate of rewiring.\cite{network_rewiring} In this project, two simple rewiring techniques are used: single link rewiring and pairwise rewiring.\\
\begin{itemize}
    \item Starting with an edge $(u,v)$; with starting node $u$ and ending node $v$. Single link rewiring will look for a node $w$ that hasn't yet been connected to node $u$. Once $w$ is found, edge $(u,v)$ will be removed and a new edge $(u,w)$ will be added to the network. 
    \item Starting with two edges $(u,v)$ and $(x,y)$. Pairwise rewiring will remove both edges, and two new edges will be added: $(u,y)$ and $(x,v)$.
\end{itemize}

Single link rewiring tends to give higher randomness to the network, and pairwise rewiring preserves the degree distribution. Both rewiring techniques require a parameter $p$, which is the probability of rewiring for each edge. If $p=1$, using single link rewiring will cause the network to become a random network. However, since pairwise rewiring preserves the degree distribution, $p=1$ will not cause the network to become completely random.

\subsection{Small-world}
\label{small_world}

\begin{figure}[h]
    \includegraphics[width = 0.8\textwidth]{small_world_network_model.eps}
    \centering
    \caption{A demonstration of WS model(top) and NW model(bottom). The paramters are: $n=20$, $k=4$, $p=0,0.1,0.5$.}
    \label{fig:small_world_models}
\end{figure}
About 50 years ago, Standley Milgram\cite{milgram1967small} carried out a famous study in the interest of this question: how many intermediates are needed to pass a message between two irrelevant or distanced people? This is known as the small-world problem. As counter-intuitive as it may seem, the medium number of intermediates needed is only 5(an average of 6)\cite{milgram1967small}. This is not an undoubtedly experiment but it is almost impossible to determine the actual number of messengers needed in the real world. Nevertheless, this number would be smaller than most peoples' expectations. Mathematically, the small world problem is the study of graphs with small average distance, because network scientists believe that in a real world network, the average distance is small with a high average clustering coefficient. Previously, we introduced the formula to calculate the average distance $L_r$ of a random graph. Thus, if a graph has $L/L_r <1$ and $C/C_r>1$, this graph can be considered to have the small-world property.\\
\par
A small-world network can be generated using a Watts-Strogatz(WS) model\cite{wsmodel} or a Newman-Watts(NW) model(a variant of the WS model)\cite{nwmodel}. Both models require three parameters: number of nodes $n$, number of connected closest neighbours $k$ and rewiring probability $p$. The key of both models is rewiring. The graph starts with $n$ nodes, each node is connected to $k$($k-1$ if $k$ is odd) nearest neighbours; $nk/2$ edges will be created. For each edge, there is a probability $p$ that this edge will be rewired to another ending node(single link rewiring). While rewiring, the WS model removes the edge $(u,v)$ and add a new edge $(u,w)$. Thus, the number of edges stays the same. However, the NW model maintains the edge $(u,v)$ and adding the new edge $(u,w)$, causing the expectation of the number of edges after rewiring to be $nk/2+pnk/2$. Rewiring will add short paths to the network, and cause the average distance to be exceptionally smaller. Suggested by Barabási\cite{barabási2016network}, to obtain both high clustering and low average distance(properties of a small-world network), $p$ should be between 0.001 and 0.1.\\


\subsection{Scale-free network}
A controversial topic of network science is whether
real networks are usually scale-free\cite{holme2019rare}. To understand what is a scale-free network, we need to scope into the degree distribution of graphs.\\

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{degree_distribution.eps}
    \centering
    \caption{Degree distribution of a $G(n,m)$ graph with $n=1000,m=5000$ and a graph generated using Barabási-Albert model with $n=1000,m=5$.}
    \label{fig:degree_dist}
\end{figure}

Suggested by Barabási\cite{barabási2016network}, the degree distribution of a random network is expected to follow a binomial distribution. However, it might not be the actual distribution of real networks. A controversial idea that hasn't yet been proven in the network science community is: are real networks' degree distribution follows a power-law distribution? A power-law distribution follows: $P(k) \sim k^{-\gamma }$, the parameter $\gamma$ is typically in the range $2<\gamma<3$. If the degree distribution of a network follows a power-law distribution, the network is said to be a scale-free network. Even though there are counter-examples\cite{broido_clauset_2019}, many network scientists still believe that real networks are scale-free\cite{albert1999diameter}. In order to further simulate the behaviour of real networks, Barabási introduced the Barabási-Albert(BA) model to create scale-free networks.\cite{barabási2016network}.\par
The BA model requires two parameters: $n$ and $m$. Initially, only one node is created and $n-1$ nodes will be added to the network consequently. Whenever a node is added to the network, it will connect $m$ nodes. The logic of connection is the key to this model. Nodes are more likely to link to nodes with more edges than nodes with less edges. For instance, a node has been added to the network, this node will connect to a node with degree $k=7$ with a higher probability than a node with degree $k=3$. This logic of connection is called preferential attachment. Essentially, like in the real world, nodes are more likely to connect to nodes that have more impact on the network.\cite{pa_test} The BA model ensures most of the nodes have low degrees, whereas only a few nodes have exceptionally high degrees, as shown in figure \ref{fig:degree_dist}. An ideal way to fit the power-law distribution is using the linear regression technique to fit the data in log-log scale.

\subsection{Generating graphs}
\label{generate_graphs}
All graph models are utilized as below(unless specified):
\begin{itemize}
    \item $G(n,m)$ random graphs/networks or random graphs/networks. With given $n$, $m$ will be randomly selected between $n-1$ to $n(n-1)/2$ to create various networks with different number of $m$.
    \item For WS and NW graphs, three parameters are required. Given parameter $n$, $k$ will be randomly selected between 1 and $(n-1)$ to give a larger range of $m$. $p$ is also randomize, within the range 0.01 and 0.1 to simulate a small-world network as mentioned in section \ref{small_world}. By randomize two parameters, more diverse samples will be generated.
    \item BA graphs require $n$, $m$ as parameters. They are randomised between 1 and $(n-1)$, so we can generate samples with a different number of edges.
\end{itemize}

\section{Methods}
\subsection{Implemeted methods}
In this project, 8 methods will be introduced, 7 of them are based on paper published\cite{KIM20082637}\cite{odc}, and a new measure $MA_{RI}$ will be introduced. $MA_{RI}$ is based on the idea of $MA_g$, and it also performs similarly.\par
To compare complexities, all complexities should be normalised within range $0<complexity<1$. Hence, finding the upper bound and lower bound of each complexity is essential, even though the lower bound for most measures is 0.
\begin{itemize}
    \item Subgraph measures:
    \begin{itemize}
        \item $C_{1e,st}$
        \item $C_{1e,spec}$
        \item $C_{2e,spec}$
    \end{itemize}
    \item Product measures:
    \begin{itemize}
        \item $MA_g$
        \item $MA_{RI}$
        \item $Cr$
        \item $Ce$
    \end{itemize}
    \item $OdC$ (Entropy measure)
\end{itemize}
\subsection{Different subgraph measures}
Definition of a subgraph was introduced in section \ref{graph}, so an intuitive idea of measure is to count the number of different subgraphs in a given graph. The question is: how to count subgraphs and how to determine whether they are identical or not? Different subgraphs can be created using edge deletion. There are two approaches in this project: removing one edge or remove two edges. Removing more edges is not optimal considering the complexity of calculations. Deleting one edge from a graph with $m$ edges will result in $m$ subgraphs, or ${m \choose 2}$ using two edges deletion. After obtaining $m$ or ${m \choose 2}$ subgraphs, the next step is to find the number of unique subgraphs. Two identical subgraphs are said to be isomorphic. To determine isomorphism, there are two possible methods.\par
The first approach is to find out the number of spanning trees of a subgraph. A spanning tree contains all the nodes in a subgraph with the minimum number of edges. This can be calculated using Kirchkoff's theorem\cite{priezzhev1985dimer}: number of spanning trees = cofactor of Laplacian matrix $L$. By counting different numbers of spanning trees, isomorphism can be determined: two subgraphs with the same $m$, $n$ and the same number of spanning trees are isomorphic. This is a computationally cheaper approach with possible errors. A more complex but precise way to determine isomorphism is to compare the spectrum of the Laplacian matrix of subgraphs. Calculating and comparing both the Laplacian matrix and the signless Laplacian matrix(all entries are positive or 0) will offer a better result. If two subgraphs with the same spectrum on both the Laplacian matrix and signless Laplacian matrix, they are isomorphic. As mentioned, the complexity needs to be normalised to allow comparison. A good upper bound for both edge deletion method is $m_{cu}=n^{1.68}-10$.\cite{KIM20082637}.Three measures can be defined:
\begin{itemize}
    \item $N_{1e,st}$, the number of different subgraphs with different number of spanning trees, after cutting one edge. The normalised complexity $C_{1e,st} = (N_{1e,st}-1)/(m_{cu}-1)$.
    \item $N_{1e,spec}$, the number of different subgraphs with different spectrums, after cutting one edge. The normalised complexity $C_{1e,spec} = (N_{1e,spec}-1)/(m_{cu}-1)$.
    \item $N_{2e,spec}$, the number of different subgraphs with different spectrums, after cutting two edges. The normailised complexity $C_{2e,spec} = (N_{2e,spec}-1)/({m_{cu} \choose 2}-1)$.
\end{itemize}

\subsection{Potential problems and solutions of different subgraph measures}
\label{problem}
Problems were found during the implementation of different subgraphs. Possible solutions are also given for future studies.\\
Different subgraph measures are principally simple, but they are complex to compute, within at least $O(n^2)$ time\cite{KIM20082637}. This is not the only problem. An upper bound of the complexity $m_{cu} = n^{1.68}-10$ was assumed by Kim and Wilhelm\cite{KIM20082637} to normalise the complexity. However, from the simulation, we found that this may not be the actual upper bound of the different subgraph measures.

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{subgraph_measures.eps}
    \caption{Different subgraph measure of 1000 $G(n,m)$ random graphs, with $n$ = 15.}
    \label{fig:subgraph_measure}
\end{figure}

The complexity is abnormal for graphs with around 90 edges and 15 nodes as shown in figure \ref{fig:subgraph_measure}. These exceptions could imply that the upper bound assumption $m_{cu}$ is not correct, but there is another possible reason, which is the problem of the floating-point arithmetic.\par
On most machines today, numbers are represented in binary system\cite{floating_point}. For example, 0.2 is recorded as 0.00110011001100110011... in a binary system. This series is infinite, represented by $1*2^{-3}+1*2^{-4}+1*2^{-7}+1*2{-8}...$. For obvious reasons, computer scientists don't want to work with infinite series. Therefore, the series is approximated. On a modern computer, the series is usually approximated to 63 digits, and an extra one digit represents the sign of the number. After approximation, the error could cause the equal operation to fail. A famous example is that for modern programming language or machine that operates this numbering system, 0.2+0.1 does not equal 0.15+0.15. As a result, the inaccurate comparison may cause mistakes and generates more different subgraphs than actual.\par
The core of different subgraph measure is to compare the cofactor ($C_{1e,st}$) or spectrum ($C_{1e,spec}$ and $C_{2e,spec}$) of a subgraph. Given the fact that the probability of a decimal number to appear in the spectrum is high, and the cofactor might exceed the upper bound of the numbering system\cite{python_int}. The comparisons will be inaccurate with a high probability. There are three possible solutions:
\begin{itemize}
    \item As suggested, errors will be made when approximated by the machine. An error threshold can be used when comparing spectrums and cofactors. For example, two numbers with relative error less than 1\% can also be considered equal numbers. One disadvantage is the increase of complexity, taking more time and effort to compare the spectrums/number of spanning trees.
    \item Similarly, numbers can be rounded before comparison to avoid errors. This is used in the implementation of different subgraph measures, all cofactors and spectrums are rounded to the first ten significant figures. This solution requires less computation time than the first solution and reduce the number of misclassification. The drawback is that similar graphs can be considered as isomorphic graphs, this disadvantage also applies to the first provided solution, but with higher accuracy for large graphs. This solution may still cause complexities larger than 1, but it is the best solution considering the effort spent. Thus, this solution will be used in this project.
    \item Instead of using $m_{cu}$ as the upper bound, $m$ or $n(n-1)/2$ can be used for one-edge-deleted subgraph complexity and $\genfrac(){0pt}{2}{m}{2}$ for two-edges-deleted subgraph complexity. This guarantees the normalisation and avoids the mistakes that are caused by the first two solutions. On the other hand, causing the complexity to be different.
\end{itemize}
\par
A unique problem with $C_{2e,spec}$ is the value is not properly scaled for small graphs.

\begin{figure}[ht]
    \includegraphics[width = \textwidth]{c2espec.eps}
    \caption{$C_{2e,spec}$ complexities of $G(n,m)$ random graphs for n = 6,7,8 respectively. For each $m$, 50 samples are generated}
    \label{fig:c2espec}
    \centering
\end{figure}
\par
The upper bound of $C_{2e,spec}$ is 0.5 while $ n\leq7 $. To have an upper bound at 1, the complexity values have to be scaled by 2. However, scales by two will cause the complexity to exceed 1 for larger graphs. Thus, we stick to the original normalisation and scaling: $C_{2e,spec}$ will have an upper bound at 0.5 for $n=7$, and less for smaller $n$s.

\subsection{Product measure}
The core idea of product measure is to assign low complexity at both extremes(path and cliques) and higher complexity to graphs with a medium number of edges.
\subsubsection{Medium articulation for graphs($MA_g$)}
\label{magdefinition}
This measure is based on redundancy and mutual information of a graph.\cite{wilhelm2007information} The complexity before normalisation can be defined as the product of redundancy and mutual information $C=R*I$, with redundancy $R=\frac{1}{m}\sum_{i,j>i}ln(d_id_j)$ and mutual information $I=\frac{1}{m}\sum_{i,j>i}ln(\frac{2m}{d_id_j})$. The core of this measure is the variable $d_id_j$. It is product of degree of two nodes. This variable will be maximised when the graph is fully connected(clique) and minimized when the graph is least sparse and highly ordered(path).
\begin{itemize}
    \item Highest redundancy: $R_{clique} = 2ln(n-1)$
    \item Lowest redundancy: $R_{path} = 2(\frac{n-2}{n-1})ln(2)$
    \item Highest mutual information: $I_{path} = ln(n-1)-(\frac{n-3}{n-1})ln2$
    \item Lowest mutual information: $I_{clique}=ln(\frac{n}{n-1})$
\end{itemize}
\par
Since the maximum and the minimum of redundancy and mutual information can be found, the normalisation technique can be applied to ensure the complexity $MA_g$ is bounded by 0 and 1.
\begin{equation}
    MA_g=MA_R*MA_I,
\end{equation}
with:
\begin{equation}
    MA_R=4(\frac{R-R_{path}}{R_{clique}-R_{path}})(1-\frac{R-R_{path}}{R_{clique}-R_{path}})
\end{equation}
\begin{equation}
    MA_I=4(\frac{I-I_{clique}}{I_{path}-I_{clique}})(1-\frac{I-I_{clique}}{I_{path}-I_{clique}})
\end{equation}
\subsubsection{Efficiency complexity($Ce$)}
Efficiency complexity is based on the efficiency of adding shorter path in a network.\cite{latora2003economic} As suggested in section \ref{small_world}, adding short paths will significantly shorter the average shortest distance in a network. However, adding more paths will cost energy. The ratio between shorten distances and cost of energy is defined as the efficiency. A complex graph should maintain short average distance and low energy used as possible. The efficiency is calculated as $E=\frac{1}{n(n-1)/2}\sum_{i,j}\frac{1}{d_i,j}$. To normalise the complexity, identify the lowest graph with lowest efficiency, a path, is: $E_{path} = 2/(n(n-1))\sum_{i=1}^{n-1}(n-i)/i$. Consequently, the normalised complexity is $C_e=4(\frac{E-E_{path}}{1-E_{path}})(1-\frac{E-E_{path}}{1-E_{path}})$

\subsubsection{Graph index complexity($Cr$)}
In section \ref{graph}, Laplacian matrix was introduced. $Cr$ is depending on another graph matrix which is called adjacency matrix. The adjacency matrix is essentially Laplacian matrix with all diagonal elements being 0. This records all the edges within the network. Using previous studies, all eigenvalues of the adjacency matrix are real.\cite{brouwer2011spectra} The largest eigenvalue is also knows as the index $r$. To normalise the complexity, $r$ is found out that $2cos(\pi /(n+1) \leq r \leq n-1)$.\cite{brouwer2011spectra} Therefore, a normalised complexity can be constructed, which is given by: $Cr=4c_r(1-c_r)$ with $c_r=\frac{r-2cos(\pi /(n+1))}{n-1-2cos(\pi /(n+1))}$.

\subsection{Offdiagonal complexity($OdC$)}
A node-node link correlation matrix need to be constructed to calculate $OdC$.\cite{odc} The matrix element $c_{i,j}$ records the number of neighbours with degree $j \geq i$ of all nodes with degree $i$. $OdC$ assigns higher complexity to graphs that their nodes have no preferences on their neighbours degree. A vector $a_n$ is calculated by summing up all the rows in the matrix. The normalised $OdC$ is calculated as:\\
$OdC=-(\sum^{k_{max}-1}_{n=0}A_nln(A_n))/ln(n-1)$ with $A=a_n/\sum_{n=0}^{k_{max}-1}a_n$(probability of each $a_n$).
\subsection{$MA_{RI}$}
$MAg$ measure is a product measure, which assigns higher complexity to graphs with medium number of edges and lower complexity at both extremes (path and clique). Using the product of redundancy $R$ and mutual information $I$, with normalisation, $MAg$ is defined as\cite{KIM20082637}:
\begin{equation}
    \label{eq:RandI}
    \begin{gathered}
        R = \frac{1}{m}\sum_{i,j>i}ln(d_id_j)\\
        I = \frac{1}{m}\sum_{i,j>i}ln(\frac{2m}{d_id_j})\\
    \end{gathered}
\end{equation}
\begin{equation}
    \label{eq:mag}
    \begin{gathered}
        MA_R = 4(\frac{R-R_{path}}{R_{clique}-R_{path}})(1-\frac{R-R_{path}}{R_{clique}-R_{path}})\\
        MA_I = 4(\frac{I-I_{clique}}{I_{path}-I_{clique}})(1-\frac{I-I_{clique}}{I_{path}-I_{clique}})\\
        MA_g = MA_R * MA_I
    \end{gathered}
\end{equation}
$I$ can be written as:
\begin{equation}
    \begin{gathered}
        \label{eq:rewriteI}
        I = \frac{1}{m}\sum_{i,j>i}ln(\frac{2m}{d_id_j})\\
        I = \frac{1}{m}(\sum_{i,j>i}ln(2m)-\sum_{i,j>i}ln(d_id_j))\\
        I = \frac{1}{m}\sum_{i,j>i}ln(2m)-\frac{1}{m}\sum_{i,j>i}ln(d_id_j)\\
    \end{gathered}
\end{equation}
\begin{equation}
    \label{eq:mutual_info}
    I = ln(2m)-R
\end{equation}
\par
$R_{path},R_{clique},I_{path}$ and $I_{clique}$ represent the lowest redundancy, highest redundancy, highest mutual information and lowest mutual information of graphs with fixed $n$ respectively. The equations can be found in section \ref{magdefinition}. Kim and Wilhelm suggested that network scientists may use $C=(R-R_{path})(I-I_{clique})$ as a complexity measure, however, the upper bound cannot be found to normalise the complexity \cite{KIM20082637}. From our study, an upper bound of $C$ can be calculated analytically.\par
Assuming the upper bound $C_{max}$ can be found, $0<C/C_{max}<1$. As suggested in equation \ref{eq:mutual_info}, $I = ln(2m)-R$, we can rewrite the complexity equation:\\
\begin{equation}
    \label{eq:rewrite_complexity}
     C = (R-R_{path})(ln(2m)-R-I_{clique})\\
\end{equation}
\begin{equation}
    \label{eq:rewrite_complexity_1}
    C = -R^2+(ln(2m)-I_{clique}+R_{path})R+(-R_{path}ln(2m)+R_{path}I_{clique})
\end{equation}
\par
As shown in equation \ref{eq:rewrite_complexity_1}, the complexity function is a quadratic function with only variable $R$, which denotes, there is one and only one extreme. As a product measure, the extreme should be a maximum. To find the maximum, differentiates the function with respect to $R(R_{max})$ where the function's slope is 0:
\begin{equation}
    \label{eq:mari_diff}
        \frac{dC}{dR} = -2R_{max}+ln(2m)-I_{clique}+R_{path} = 0\\
\end{equation}
\begin{equation}
    \label{eq:rmax}
    R_{max} = \frac{ln(2m)-I_{clique}+R_{path}}{2}
\end{equation}
\par
Even without assumption, $d^2C/dR^2 = -2$ implies the extreme is a maxima. As $C_{max}$ is found, substitutes equation \ref{eq:rmax} into equation \ref{eq:rewrite_complexity}:\\
\begin{equation}
    \begin{gathered}
        C_{max} =(R_{max}-R_{path})(ln(2m)-R_{max}-I_{clique})\\
        C_{max} = (\frac{ln(2m)-I_{clique}+R_{path}}{2}-R_{path})(ln(2m)-\frac{ln(2m)-I_{clique}+R_{path}}{2}-I_{clique})\\
        C_{max} = (\frac{ln(2m)-I_{clique}-R_{path}}{2})(\frac{ln(2m)-R_{path}-I_{clique}}{2})\\
    \end{gathered}
\end{equation}
\begin{equation}
    \label{c_max}
    C_{max} = \frac{(ln(2m)-I_{clique}-R_{path})^2}{4}
\end{equation}
\noindent
Thus, using equation \ref{c_max}, a new normalized measure $MA_{RI}$ can be defined using $C/C_{max}$:\\
\begin{equation}
    MA_{RI} = \frac{4(R-R_{path})(I-I_{clique})}{(ln(2m)-I_{clique}-R_{path})^2}
\end{equation}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{mariandmag.eps}
    \caption{$MA_g$ and $MA_{RI}$ complexity of $G(n,m)$ networks, all networks have 20 nodes and 1000 samples with random $m$ have been generated.}
    \label{fig:marimagcompare}
\end{figure}
\par
As shown in figure \ref{fig:marimagcompare}, $MA_{RI}$ gives higher coplexity to sparser graphs but less complexity when approaching to medium number of links. Additionally, it decreases more linear and smoothly compare to $MA_g$.

\section{Result}
\subsection{Complexity measures on small random graphs}
\label{complexity}
To validate all the implemented measures, all measures are applied on $G(n,m)$ random graphs, with $n=7$, and 50 samples are generated for each $m$. As shown in figure \ref{fig:small_graphs}, we reproduced results as Kim and Wilhelm did in \cite{KIM20082637}. Except $C_{2e,spec}$, as mentioned in section \ref{problem}, there is a scaling problem. Most of the methods reach their maximum briefly before the medium number of edges. Low complexities are given to highly connected graphs and sparse graphs as expected.\par
\clearpage
\newpage
\begin{figure}[p!]
    \includegraphics[width=0.85\textwidth]{complexities.eps}
    \vspace*{-0.8in}
    \centering
    \caption{Complexity of graphs generated using $G(7,m)$ model, 50 samples are generated for each $m$. Methods from top-left to bottom-right are: $C_{1e,st}$, $C_{1e,spec}$, $C_{2e,spec}$, $OdC$, $MAg$, $Cr$, $Cr$ and $MAri$.}
    \label{fig:small_graphs}
    \clearpage
\end{figure}
\noindent
\newpage
\begin{figure}[p!]
    \centering
    \vspace*{-2in}
    \includegraphics[width = 0.85\textwidth]{complexities_sp.eps}
    \vspace*{-0.8in}
    \caption{Complexity of 500 $G(n,m)$ graphs, 100 BA graphs, 100 WS graphs and 100 NW graphs, with $n=20$. Graphs are generated according to section \ref{generate_graphs}.}
    \label{fig:graph_models}
\end{figure}

Different subgraph measures perform similarly when the graph is neither sparse nor strongly connected, there is a big difference between the maximum and minimum with the same $m$. Thus, it is difficult to predict the complexity of a graph with given $m$ and $n$. The highest complexity is reached at $m=15$ for $C_{1e,st}$ and $C_{1e,spec}$ and $m=14$ for $C_{2e,spec}$. There is a miss in the plot: $C_{1e,spec}$ and $C_{2e,spec}$ plot does not contain a data point at (6,0). There is a very small probability for $G(n,m)$ model to generate a star graph(n-1 nodes are connected to 1 node, in total of n-1 edges), which will result in 0 complexity using $C_{1e,spec}$ and $C_{2e,spec}$ measure. Uses different subgraph measures for small graphs is optimal as it is relatively independent of $m$, but not for large graphs due to its complexity.\par


$OdC$ is based on the node-node link correlation matrix of a graph.\cite{odc} Thus, it spreads across the space and has little relationship with $m$. $OdC$ assigns a lot of graphs with six edges high complexity than desired. $OdC$ is "hierarchy sensitive", it may not creates a big difference between graphs when the graphs are considerably small.\par

All 4 product measures are similar, gives higher complexity value at a medium number of edges and less at both extremes. There is a very small difference between graphs with the same number of edges. $Ce$ and $Cr$ tend to give the highest complexity to graphs with exactly $n(n-1)/4$ edges, and $MA_{RI}$ and $MA_{g}$ reach their maximum before a medium number of edges as expected. Product measures are highly dependent on $m$, such that the complexity of a graph can be approximated solely based on $m$ and $n$. Network scientists may use machine learning techniques to estimate the complexity of a graph using $m$ and $n$ in a small amount of time. On the other hand, product measure may not be optimal because a complexity measure should not solely base on $m$ and $n$, but the overall structure of a network.


\subsection{BA,WS and NW model}
As informed in section \ref{problem}, different subgraph measures have normalisation problems and the complexities would exceed one.\par
Surprisingly, different subgraph measures and product measures are struggling to separate random graphs, WS graphs, and NW graphs. Only $OdC$ separates random graphs and WS, NW model by giving random graphs higher complexity than WS and MW model with fixed $m$. This is because $OdC$ awards graphs with complicated degree correlations.\\
More fascinating results are obtained from BA graphs. Different subgraph measures assign lower complexity to BA graphs compare to random graphs. This can be caused by preferential attachment. Preferential attachment ensures most nodes have low degrees and builds hubs(nodes with high degrees) in the graph. After cutting an edge/two edges between hubs and nodes with small degrees, there is a high chance an isomorphic subgraph can be found, thus lower the complexity of the graph. Specifically, subgraphs resulted by cutting the edge between hubs and nodes with small degrees are very similar and occasionally isomorphic.\par
$MA_g$ and $MA_{RI}$ perform similarly by assigning BA graphs lower values towards a medium number of links. Both measures are depending on the variable $\sum_{i,j>i}d_id_j$. BA graphs usually have less $\sum_{i,j>i}d_id_j$ because they are highly structure, causing less sum than a random graph. Contrarily, $Ce$ and $Cr$ cannot distinguish BA graphs and random graphs. $Ce$ is based on the efficiency of a graph, a highly complex graph should have a small average distance with not too many edges simultaneously. BA graph does not perform differently than random graphs in $Ce$ measure.\\

\subsubsection{Configuration model}
\label{configuration_model}
As introduced in section \ref{small_world}, a network is said to be scale-free if its degree distribution follows a power-law distribution with $2<\gamma <3$. To observe how the change of $\gamma$ would affect the complexity of a network, we need a model that can generate the graph with given $\gamma$. A configuration model \cite{newmanbook} is able to turn a given degree distribution into a graph, which has the exact degree distribution as the given degree series. 
\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{configuration_model.eps}
    \caption{Complexities of graphs generated using configuration model, $n=50$. Results are average of 50 simulations.}
    \label{fig:configuration_model}
\end{figure}
\par
$OdC$ and $C_{1e,spec}$ didn't change much by varying $\gamma$. $MA_{RI}$ increases slightly as it is very sensitive to the change of degrees of nodes. Overall, varying $\gamma$ does not impact the complexity by a lot. Due to limitation of our implementation, we can only generate a degree distribution within the range $2.7<\gamma<3.3$, by varying $n$ and $\gamma$ in a larger range, and adding a new variable $n$ in the model, it can be more inspirational. 

\subsection{Complexity correlation}
Different type of measures focus on different properties/parameters of a network, monitor the correlation between measures enable further understanding of the properties/parameters each measure is highlighting on. Additionally, network scientists may use more than one complexity measure on a network to determine whether they are truly "complex" or not. Combining complexity measures on networks will allow network scientists to comment on the network complexity from a different aspect. For these reasons, we choose three measures($C_{1e,st}$,$OdC$ and $MA_{RI}$) and monitored their behaviours on random and BA graphs. The reason BA graphs are added is to investigate whether distinguish BA graphs and random graphs is possible or not.\\
\begin{figure}[h!]
    \makebox[\textwidth]{
    \includegraphics[width = \textwidth]{complexity_correlation.eps}
    }
    \caption{Correlation of complexity measures on 1000 random graphs and 200 BA graphs with $n=25$. Generated according to the rules stated in section \ref{generate_graphs}. Colorbar represents change of $m$: higher the $m$, darker the data points.}
    \label{fig:correlation}
\end{figure}

\noindent
As figure \ref{fig:correlation} suggests, the correlation between complexity measures are difficult to observe, since they are trying to address different things. As $C_{1e,st}$ value increases, $OdC$ also increases. We indicated in section \ref{complexity}, $m$ is not an important factor for $OdC$. On the other hand, $C_{1e,st}$ is highly based on $m$, the complexity increases linearly with $m$, and then complexity drops quickly after reaching the maximum. By constructing a correlation between $C_{1e,st}$ and $OdC$, separate BA graphs and random graphs are not possible.\par
Since both complexity measures are heavily dependent on $m$, thus we can observe a linearly decreasing trend. We can detect an unique distribution of the BA graphs' complexity: a shape between ellipse and half-moon. This is because both $MA_{RI}$ and $C_{1e,st}$ assign lower complexity values to some of the BA graphs, compare to other BA graphs with same number of edges. Therefore, distinguish Ba graphs and random graphs is difficult.\par
The correlation between $OdC$ and $MA_{RI}$ of the random graph seems simple: as $m$ decrease, $MA_{RI}$ decrease speedily but $OdC$ change unnoticeably until the graph is highly connected. On the other hand, the correlation between $OdC$ and $MA_{RI}$ on BA graphs perform slightly differently. As suggested, $MA_{RI}$ distributes lower complexity to some BA graphs, and $OdC$ is very consistent. In section \ref{configuration_model}, we did not observe a big change of complexity by varying $\gamma$. To observe a better result, using a larger graph is optimal(for an instance, $n=1000$), but due to technical issues(time complexity), they are not used here.



\subsection{Complement graphs}
The definition of a complement graph is fairly simple: the complement graph contains all the edges that are not in the original graph. To ensure complexity measures can be successfully applied, graphs that will cause the complement graphs to be disconnected will be removed. Thus, in theory, the original graphs and the complement graphs are not extreme graphs. Analysing the complexity correlation between the original graph and the complement graph gives us more inspiration of the measures. We have choose three different measures with different types: $OdC$, $MA_{RI}$ and $C_{1e,spec}$.
\begin{figure}[h!]
    \makebox[\textwidth]{
    \includegraphics[width = \textwidth]{complement.eps}
    }
    \caption{Complexities of the original graphs and complement graphs with $n=20$. There are two types of graphs are generated: 500 $G(n,m)$ random graphs and 100 BA graphs.}
    \label{fig:complement}
\end{figure}
\noindent
$OdC$ seems to be very symmetric with respect to $m$. Since taking the complement graph is reversing the degree distribution, the consistency of node-node link relation is preserved. Causing the complexity value for the original graph equal to the complement graph. As mentioned $MA_{RI}$ is highly based on the number of edges, so observing a linear function is not a surprise. This also applies to BA graphs. Similar to $MA_{RI}$, $C1_{1e,st}$ is keener to $m$, causing another negative correlation between the original graph and the complement graph.\\


\subsection{Applying $MA_{RI}$ on real networks}
To test the new measure $MA_{RI}$, 6 real networks are collected. To ensure comprehensive evaluation, various types of networks are used. To be applicable, bus networks in 6 cities are collected and converted to networks from raw. Bus networks are different to other real networks. Bus networks contain an extraordinary amount of nodes with degree 2. For simplification and general interest, bus networks are modified. In modified bus networks, all nodes with $k=2$ are removed, whereas the edges are preserved. For instance, node $b$ is only connected to $a$ and $c$. Node $b$ will be removed from the network and a new edge $(a,c)$ will be added to the network. The modification will be iterated multiple times until bus networks are free from nodes with degree $k=2$. The modification will significantly decrease the distance of bus networks. Moreover, generated graphs are also added to be evaluated and compared.\par
To be mentioned, for convenience, real networks are referred to non-bus networks.

\begin{table}[ht]
    \noindent\makebox[\textwidth]{
        \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
            \hline
            Label & Name & Type & n & m& L & $L_r$ &$MA_{RI}$&$OdC$\\
            \hline
            \multicolumn{9}{|c|}{Real networks}\\
            \hline
            1 &  Dolphins\cite{dolphins} & Animal interaction & 62 & 159 & 3.357 & 2.524 & 0.999 & 0.517\\
            \hline
            2 &  PDZBase \cite{pdzbase}& Protein interaction & 161 &209 & 5.326 & 5.326 & 0.824 & 0.310\\
            \hline
            3&  Hamsterster\cite{hamster} & Online social network & 874 & 4003 & 3.217 & 3.058 & 0.963 & 0.532\\
            \hline
            4& Roget's Thesaurus \cite{roget}& Synonym network &994 & 3640 & 4.075 & 3.466 & 0.960 & 0.392\\
            \hline
            5 & Flight\cite{flight} & Flight network & 3397 & 19230 & 4.103 & 3.350& 0.948 & 0.525\\
            \hline
            6 & UK train \cite{GBPT}& UK train network & 2490 & 4377 & 10.384&6.220 & 0.664 & 0.233\\
            \hline
            \multicolumn{9}{|c|}{Bus networks}\\
            \hline
            7&\multicolumn{2}{c|}{London\cite{GBPT}}&8653&12285&32.338&8.687&0.38&0.127\\ 
            \hline
            8&\multicolumn{2}{c|}{Paris\cite{bus_collection}}&10644&12309&47.631&11.059&0.173&0.065\\ 
            \hline
            9&\multicolumn{2}{c|}{Berlin\cite{bus_collection}}&4316&5869&33.284&8.366&0.358&0.134\\ 
            \hline
            10&\multicolumn{2}{c|}{Sydney\cite{bus_collection}}&22659&26720&36.131&11.688&0.173&0.064\\ 
            \hline
            11&\multicolumn{2}{c|}{Detroit\cite{bus_collection}}&5683&5946&70.513&11.708&0.062&0.020\\ 
            \hline
            12&\multicolumn{2}{c|}{Beijing\cite{beijing}}&9249&14058&27.891&8.214&0.441&0.167\\ 
            \hline
            \multicolumn{9}{|c|}{Modified Bus}\\
            \hline
            13&\multicolumn{2}{c|}{London}&3417&6018&18.308&6.462&0.553&0.128\\ 
            \hline
            14&\multicolumn{2}{c|}{Paris}&2762&4301&15.386&6.975&0.468&0.106\\ 
            \hline
            15&\multicolumn{2}{c|}{Berlin}&1662&2941&18.36&5.867&0.586&0.149\\ 
            \hline
            16&\multicolumn{2}{c|}{Sydney}&4834&8358&17.665&6.838&0.49&0.089\\ 
            \hline
            17&\multicolumn{2}{c|}{Detroit}&295&483&6.341&4.794&0.643&0.117\\ 
            \hline
            18&\multicolumn{2}{c|}{Beijing}&4072&8325&14.864&5.902&0.64&0.195\\ 
            \hline
            \multicolumn{9}{|c|}{Generated networks}\\
            \hline
            19&\multicolumn{2}{c|}{G(n,m) random network($n=875,m=4000$)}&875&4000&3.313&3.061&0.970&0.330\\
            \hline
            20&\multicolumn{2}{c|}{WS network($n=875,k=10,p=0.05$)}&875&4375&5.000&2.942&0.974&0.123\\
            \hline
            21&\multicolumn{2}{c|}{NW network($n=875,k=10,p=0.05$)}&875&4587&5.075&2.883&0.980&0.089\\
            \hline
            22&\multicolumn{2}{c|}{BA random network($n=875,m=5$)}&875&4350&2.922&2.950&0.999&0.358\\
            \hline
        \end{tabular}
    }
    \caption{Label,description and parameters of used networks} 
    \label{tb:descriptions}
\end{table}


\begin{figure}[ht]
    \includegraphics[width = \textwidth]{real_networks.eps}
    \caption{$MA_{RI}$ complexity of real networks, bus networks, modified bus networks and graphs generated by graph models, labelling and description can be found in table \ref{tb:descriptions}.}
    \label{fig:real_networks}
\end{figure}
\par
After careful consideration, average distance ratio $L/L_r$ is chosen to be the variable showing on x-axis in figure \ref{fig:real_networks}. Average distance is a more important parameter to consider because it can identify how "small-world" the network is. As suggested, all the bus networks have a significantly higher average distance ratio than real networks. Even after being modified, the average distance ratios are still relatively high. But higher average distance ratio brings less $MA_{RI}$ complexity. We can suggest a negative correlation between the average distance ratio and the $MA_{RI}$ complexity. There is an exception of real networks, which is the UK train network. Considering the similarity between bus networks and train networks, it is not surprising. Generated graphs also behave similar to standard real networks; low average distance ratio with high complexity, as they are intended to simulate the behaviour of standard real networks. In theory, "Flight" is also a public transport system, so it should behave similarly to bus networks or the train network. However, the "Flight" network performs, unlike transport networks. Flight networks are designed to be robust\cite{zhixing2021recent} to be functional under severe weather/accident. Unlike trains or buses, flights are less limited by physical paths and routes.\par
To discuss the cause of the increase of $MA_{RI}$ complexity after modification, we need to consider the only parameter $MA_{RI}$ focusing on(with fixed $m$ and $n$). $\sum_{i,j>i}d_id_j$ is the only variable that affects the complexity of a graph. The essence of the measure is calculating the average $d_id_j$. By removing nodes with degree 2, average $d_id_j$ will be increased and leads to an increase of the $MA_{RI}$ complexity.

\subsection{Rewiring on real networks}
As recommended in section \ref{rewiring}, rewiring is an important technique to monitor the behaviour of a network. Hence, we rewired real networks and modified bus networks to record their behaviour. Both single link rewiring and pairwise rewiring will be used. The reason we are going to use $OdC$ instead of $MA_{RI}$ is that pairwise rewiring will keep the degree relationship, and $MA_{RI}$ will stay unchanged.\par
How we rewire graphs:
\begin{itemize}
    \item The rewiring is done gradually. Initially, graph $G$ will be rewired with probability $p=0.05$. After recording the results, we rewire it with probability $p=0.05$ again. This step will be repeated 20 times.
    \item Both rewiring have been simulated on all graphs more than ten times("dolphins" and "PDZBase" have been rewired more than 100 times, as they are relatively small) to generate more consistent results.
\end{itemize}
\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{rewiring.eps}
    \caption{Change of $OdC$ complexities respect to change of rewiring probability with an error bar(one standard deviation).}
    \label{fig:rewiring}
\end{figure}
\noindent
From figure \ref{fig:rewiring}, we can conclude that most real networks behave similarly using rewiring. As mentioned in section \ref{rewiring}, single link rewiring results in higher randomness, because it destroys the degree distribution. In theory, when $p=1$, the graph becomes a random network. $OdC$ decreases significantly with the increase of rewiring probability for real networks, whereas the change of complexity is not large for pairwise rewiring. This is because $OdC$ is highly sensitive to degree relationship, but pairwise rewiring does not change the degree relationship. Also, the error bar for "dolphins" and "PDZBase" is large. This is because they are small networks, rewiring will make a larger impact than larger graphs.\par
There is an exception in real networks; the UK train network performs similar to bus networks rather than real networks. As public transportation networks, single link rewiring will cause the complexity to increase. Generally, the increase of complexity becomes small after $p=0.4$; almost half of the links have been rewired. As shown in table \ref{tb:descriptions}, $OdC$ complexities are very low for all the bus networks. As introduced by Claussen\cite{odc}, $OdC$ assigns high complexity to graphs that have no preference for the degree of their neighbours. After destroying the degree correlation using single link rewiring, the $OdC$ complexity will increase.\par
In contrast, pairwise rewiring will cause the complexity to decrease briefly like real networks, with similar reasons.

\section{Conclusion and further study}
In this thesis, we investigated and compared different measures. The different complexity measures focus on different properties and parameters of a graph. For example, different subgraph measures focus on the general structure of subgraphs, lead to high complexity, and they are not feasible to apply different subgraph measures on large networks. In addition, the floating-point arithmetic is imperfect when dealing with large numbers and decimals.
On the other hand, product measures are fairly simple in terms of calculation time. Especially $Cr$, the easiest complexity measure, which can be calculated in $O(n)$ time\cite{KIM20082637}. A drawback of product measures is that they are highly based on $m$, creating smaller differences than other complexity measures with fix $m$ and $n$.
$OdC$ measure can distinguish random graphs and graphs generated using BA,WS and NW model, and within a relatively small amount of time. However, the border of complexity value are wide. We suggest using the measure that is most suitable, depending on which specific parameters or properties are most important in the problem.\par
Additionally, we constructed a new measure $MA_{RI}$ based on the idea of $MA_g$. $MA_{RI}$ assigns higher complexity to sparser graph than $MA_g$. To be noticed, to calculate $MA_{RI}$, $R_{clique}$ and $I_{path}$ are not required, but $m$ is involved in the calcualtion.\par
We also compare the difference between real networks and bus networks, and we investigated the unique property of bus networks: high average distance with low complexity, which is the opposite of a small-world network. This causes average lower complexities than other real networks.\par
There are more complexity measures \cite{emmert-streib_dehmer_2012}\cite{dehmer_barbarini_varmuza_graber_2009} for further studies and researches. We encourage studies and applications of the measures on the different types of graphs and monitor the behaviour of different complexity measures. Further studies on complexity measures could help the network science community to build a comprehensive, robust and applicable complexity measure that everyone can agree on.\par
An unexpected fact was observed: the degree-based measures($OdC$,$MA_g$,$MA_{RI}$) generally assign relatively high complexity to very sparse graphs, except $Ce$. We are not sure whether this is a coincidence or not, but this can be a topic to further investigate.\par
Overall, we hope our result can stimulate more studies on network complexity measures. The definition of complexity is already difficult to be determined. A good complexity measure should be able to distinguish: random networks, small-world networks, scale-free networks and real networks. In addition, the highest complexity to graphs with the number of edges is slightly less than the medium. Our work provides useful aspects and observations to build an optimal complexity measure in the future.

\appendix
\section{Data}
All real networks' citation can be found in the reference. To analyse them, all networks required pre-processing. Using Gephi is optimal. They need to be converted into the following form: a csv file with two columns: ''source'' and ''target'', which stores all the starting nodes and the ending nodes of all the edges. All processed real networks should be placed in ''real$\_$networks$/$processed'' and processed networks in ''bus$/$processed''.

\section{utilities.py}
\label{utilities_code}
\begin{lstlisting}[language=Python,breaklines=true]
import networkx as nx
import random
from random import randint
import pandas as pd
import numpy as np
import collections
from scipy.stats import pearsonr
from itertools import combinations_with_replacement
import math
from math import log
import matplotlib.pyplot as plt


#   Generates a list of graphs and their corresponding parameter.
#   Notice the function will only return connected graphs, disconnected graphs
#   will not be returned. Thus the actual number of graphs returned will be less
#   than expected.
#   Parameters:
#   n: number of nodes
#   use_all_m: if this is true, then the function will generates samples using all
#   the edge numbers * sample_number
#   sample_number: number of samples for each edge if use_all_m = True
#   Otherwise the sample_number defines how many graphs will be returned in total
def random_networks(n=7,use_all_m = True,sample_number = 50):
    #   Create a list to record all the graphs
    graph_list = []
    #   Max number of edges.
    m = (n*(n-1))/2
    m = int(m)
    #   Create a df to store relevant information
    column_names = ["Number_of_edges","Average_degree",
                    "Average_distance",
                    "Average_clustering"]
    #   Filling the df with zeros
    if use_all_m == True:
        zero_list = [float(0)]*(sample_number*(m+1))
        df = pd.DataFrame(columns = column_names)
        for item in column_names:
            df[item] = zero_list 
    else:
        zero_list = [float(0)]*(sample_number)
        df = pd.DataFrame(columns = column_names)
        for item in column_names:
            df[item] = zero_list 
    
    #   Creating graphs using different parameters
    #   count record the actual number of graphs generated by the function
    count = 0
    if use_all_m == True:
        for i in range(m+1):
            for j in range(sample_number):
                temp_graph = nx.gnm_random_graph(n,i)
                if nx.is_connected(temp_graph):
                    graph_list.append(temp_graph)
                    df["Number_of_edges"][count] = i
                    df["Average_degree"][count] =  (2*i)/n
                    df["Average_clustering"][count] = nx.average_clustering(temp_graph)
                    df["Average_distance"][count] = nx.average_shortest_path_length(temp_graph)
                    count  +=1

    else:
        for i in range(sample_number):
            edge_number  = randint(0,m)
            temp_graph=nx.gnm_random_graph(n,edge_number)
            if nx.is_connected(temp_graph):
                graph_list.append(temp_graph)
                df["Number_of_edges"][count] = edge_number
                df["Average_degree"][count] =  (2*edge_number)/n
                df["Average_clustering"][count] = nx.average_clustering(temp_graph)
                df["Average_distance"][count] = nx.average_shortest_path_length(temp_graph)
                count +=1
    return graph_list,df


#   Return a list of subgraphs of the graph G by taking an edge off the graph

def subgraph_one_edge_deletion(G):
    subgraphs = []
    for edge in list(G.edges):
        temp_graph = G.copy()
        temp_graph.remove_edge(edge[0],edge[1])
        subgraphs.append(temp_graph)
    return subgraphs

#   Return the number of spanning trees of a graph.
#   This calculation is based on the Kirchhoffs theorem, which is:
#   number of ST = det(reduced Laplacian matrix)
#   A reduced Laplacian matrix is the Laplacian matrix with
#   a random row i and column i to be removed
def number_of_ST(G):
    L = nx.laplacian_matrix(G)
    L = L.todense()
    remove_i = randint(0,L.shape[0]-1)
    L = np.delete(L,remove_i,0)
    L = np.delete(L,remove_i,1)
    sign ,det = np.linalg.slogdet(L)
    det = sign * math.exp(det)
    det = int(det)
    return det

#   Return number of isomorphic subgraphs of a graph
#   After taking all the one-edge-deletion subgraphs,
#   investigates the number of different ST will remove isomorphic subgraphs
def isomorphic_graphs(G):
    subgraphs = subgraph_one_edge_deletion(G)
    ST_result = []
    for graph in subgraphs:
        ST_result.append(number_of_ST(graph))
    unique_ST = []
    unique_subgraphs = []
    ST_result = [str(item)[:10] for item in ST_result]
    for i in range(len(subgraphs)):
        if ST_result[i] not in unique_ST:
            unique_subgraphs.append(subgraphs[i])
            unique_ST.append(ST_result[i])
    return unique_ST

#   Generates a list of BA random graphs
#   Parameters:
#   n = number of nodes
#   sample_number = Number of samples
def BA_random_graphs(n,sample_number):
    graphs = []
    for i in range(sample_number):
        m=randint(1,n-1)
        temp_graph = nx.barabasi_albert_graph(n,m)
        if nx.is_connected(temp_graph):
            graphs.append(temp_graph)
    return graphs

#   Generates a list of WS random graphs
#   Parameters:
#   n = number of nodes
#   sample_number = Number of samples
def WS_random_graphs(n,sample_number):
    graphs = []
    for i in range(sample_number):
        p= random.uniform(0.001,0.1)
        k= random.randint(2,n)
        graphs.append(nx.watts_strogatz_graph(n,k,p))
    return graphs

#   Generates a list of NS random graphs
#   Parameters:
#   n = number of nodes
#   sample_number = Number of samples
def NW_random_graphs(n,sample_number):
    graphs = []
    for i in range(sample_number):
        p= random.uniform(0.001,0.1)
        k= random.randint(2,n)
        graphs.append(nx.newman_watts_strogatz_graph(n,k,p))
    return graphs

#   Return a list of subgraphs by taking two edges off from the graph
def subgraph_two_edge_deletion(G):
    subgraphs = []
    remove_edges = np.linspace(0,len(G.edges)-1,len(G.edges))
    remove_edge_product = combinations_with_replacement(remove_edges,2)
    remove_edge_tuple = []
    for x in remove_edge_product:
        remove_edge_tuple.append(x)
    remove_edge_final = []
    for i in range(len(remove_edge_tuple)):
        if remove_edge_tuple[i][0]!= remove_edge_tuple[i][1]:
            remove_edge_final.append(remove_edge_tuple[i])
    
    subgraphs = []
    edge_list = list(G.edges)
    for i in range(len(remove_edge_final)):
        temp_graph = nx.Graph(G)
        edge1 = remove_edge_final[i][0]
        edge1 = int(edge1)
        edge2 = remove_edge_final[i][1]
        edge2 = int(edge2)
        edge1_loc = edge_list[edge1]
        edge2_loc = edge_list[edge2]
        temp_graph.remove_edge(edge1_loc[0],edge1_loc[1])
        temp_graph.remove_edge(edge2_loc[0],edge2_loc[1])
        subgraphs.append(temp_graph)
    return subgraphs

#   Check whether a network is empty(no edges)
def empty_check(G):
    if len(G.nodes()) == 0 or len(G.edges())==0:
        return True
    else:
        return False

#   Convert a dataframe to a network
def df_to_network(df):
    source = [row[0] for index,row in df.iterrows()]
    target = [row[1] for index,row in df.iterrows()]
    G = nx.Graph()
    [G.add_edge(item1,item2) for item1,item2 in zip(source,target)]
    return G

#   Finding the giant component of a network
def gcc(G):
    Gcc = sorted(nx.connected_components(G), key=len, reverse=True)
    G0 = G.subgraph(Gcc[0])
    return G0

#    Plotting the degree distribution of a graph
def plot_deg_dist(G):
    degree_sequence = sorted([d for n, d in G.degree()],reverse = True)
    degreeCount = collections.Counter(degree_sequence)
    deg, cnt = zip(*degreeCount.items())
    deg = list(deg)
    cnt = list(cnt)
    plt.bar(deg,cnt)
    return 0

#   Convert a NetworkX graph object to a dataframe
def network_to_df(G):
    edges = list(G.edges())
    source = [item[0] for item in edges]
    target = [item[1] for item in edges]
    df = pd.DataFrame(data = {"source":source,"target":target})
    return df

#   Rewire the network G with given probability prob,using pairwise rewiring
def pairwise_rewiring(G,prob):
    rewire_number = int(prob*len(G.edges))
    G1 = G.copy()
    c = 0 
    while c != rewire_number:
        edges = list(G1.edges())
        rewire_edges = random.sample(edges,2)
        source1 = rewire_edges[0][0]
        source2 = rewire_edges[1][0]
        target1 = rewire_edges[0][1]
        target2 = rewire_edges[1][1]
        if len(set([source1,source2,target1,target2]))==4:
            if not G1.has_edge(source1,target2) and not G1.has_edge(source2,target1):
                G1.remove_edge(source1,target1)
                G1.remove_edge(source2,target2)
                G1.add_edge(source1,target2)
                G1.add_edge(source2,target1)
                if nx.is_connected(G1):
                    c = c+1
                else:
                    G1.add_edge(source1,target1)
                    G1.add_edge(source2,target2)
                    G1.remove_edge(source1,target2)
                    G1.remove_edge(source2,target1)
    return G1

#   Rewire the network G with given probability prob,using single link rewiring
def single_link_rewiring(G,prob):
    G1 = G.copy()
    rewire_number = int(prob * len(G1.edges))
    for i in range(rewire_number):
        flag = 0
        while flag ==0:
            source = random.choice(list(G1.nodes()))
            neighbors = list(G1.neighbors(source))
            remove_neighbor = random.choice(neighbors)
            G1.remove_edge(source,remove_neighbor)
            if not nx.is_connected(G1):
                G1.add_edge(source,remove_neighbor)
            else:
                options = set(list(G1.nodes)) - set(list(G1.neighbors(source)))-set([source,remove_neighbor])
                rewire_to = random.choice(list(options))
                G1.add_edge(source,rewire_to)
                flag = 1
    return G1

#   Calculates the mutual information of a graph
def mutual_info(G):
    edges = list(G.edges)
    m = len(edges)
    I = 0
    for item in edges:
        d0 = len(list(G.neighbors(item[0])))
        d1 = len(list(G.neighbors(item[1]))) 
        I = I + log(2*m/(d0*d1))
    return I/m

#   Calculates the redundancy of a graph
def redundancy(G):
    edges = list(G.edges)
    m = len(edges)
    R = 0
    for item in edges:
        d0 = len(list(G.neighbors(item[0])))
        d1 = len(list(G.neighbors(item[1]))) 
        R = R + log((d0*d1))
    return R/m

#   Return the complement of a graph
def complement_graph(G):
    edges = list(G.edges)
    nodes = list(G.nodes)
    product_list=[]
    for i in range(len(nodes)):
        for j in range(i+1,len(nodes)):
            product_list.append((nodes[i],nodes[j]))
    complement = list(set(product_list)-set(edges))
    new_G = nx.Graph()
    for item in complement:
        new_G.add_edge(item[0],item[1])
    if len(G.nodes)==len(new_G.nodes):
        if nx.is_connected(new_G):
            return new_G
    return None

#   Calculate the L_r of a graph(average distance of a random graph with given m and n)
def lr(G):
    n = len(G.nodes)
    k = 2*len(G.edges)/n
    return log(n)/log(k)
\end{lstlisting}

\section{Complexity.py}
\label{complexity_code}
\begin{lstlisting}[language=Python,breaklines=true]
import numpy as np
import networkx as nx
from math import cos,pi,log
import utilities as ut
from scipy.linalg import eig
import math
#   All complexity measure have an optional parameter normalisation
#   If normalisation = True(set by default), the normalised value will be returned
#   Otherwise, the unnormalized form will be returned

#   Calculates OdC Complexity of a graph
def OdC(G,normalisation = True):
    if ut.empty_check(G) == True:
        return 0
    else:
        #Create a degree correlation matrix, using the max degree 
        degree_sequence = sorted([d for n, d in G.degree()], reverse=True)
        max_degree = max(degree_sequence)
        degree_correlation = np.zeros((max_degree,max_degree))
        #Building the correlation matrix
        for node in list(G.nodes):
            #An array to store all the neighbors degrees
            neighbors_degree = []
            #Getting the degree of the current node
            node_degree = G.degree(node)
            #Stating all neighbors and finding their degrees
            neighbors = list(G.neighbors(node))
            neighbors_degrees_tuple=G.degree(neighbors)
            for item in neighbors_degrees_tuple:
                neighbors_degree.append(item[1])
            #For every occurence, adding one to the matrix
            for item in neighbors_degree:
                if node_degree<=item:
                    degree_correlation[node_degree-1,item-1] +=1
        #Calculating a_k
        a_k=[]
        for i in range(max_degree):
            a_k.append(sum(degree_correlation[i]))
        A = sum(a_k)
        if A !=0:
            for i in range(len(a_k)):
                a_k[i]=a_k[i]/A
        #Calculating the complexity
        complexity = 0
        for item in a_k:
            complexity -= item*ln(item)
        #Normalisation
        if normalisation == True:
            complexity = complexity/(ln(len(G.nodes)-1))
        return complexity

#   Calculates ln(x), if x=0, return 0
def ln(x):
    if x == 0:
        return 0
    else:
        return np.log(x)
    
#   Calculates Cr complexity of a graph
def Cr(G,normalisation = True):
    if ut.empty_check(G) == True:
        return 0
    else:
        if not nx.is_connected(G):
            return 0
        adj_matrix = nx.adjacency_matrix(G)
        adj_matrix = adj_matrix.todense()
        eigenvalues, eigenvectors = np.linalg.eig(adj_matrix)
        r = max(eigenvalues)
        r = r.real
        if normalisation == True:
            n = len(G.nodes)
            c_r_numerator = r-2 * cos(pi/(n+1))
            c_r_denominator = n-1-2*cos(pi/(n+1))
            c_r = c_r_numerator/c_r_denominator
            Cr_complexity = 4*c_r*(1-c_r)
            return Cr_complexity
        else:
            return r

#   Calculates Ce complexity of a graph
def Ce(G,normalisation = True):
    if not nx.is_connected(G):
        return 0
    if ut.empty_check(G):
        return -1
    else:
        E = 0
        nodes = list(G.nodes())
        n=len(nodes)
        for i in range(n):
            for j in range(i+1, n):
                E = E + 1/nx.shortest_path_length(G,nodes[i],nodes[j])
        E = 2* E /((n)*(n-1))
        if normalisation ==True:
            E_path = 0
            for i in range(1,n):
                E_path = E_path + (n-i)/i
            E_path = E_path *2/((n)*(n-1))
            Ce = 4*(E-E_path)/(1-E_path)*(1- (E-E_path)/(1-E_path))
            return Ce
        else:
            return E

#   Calculates C_{1e,st} complexity of a graph
def C1est(G,normalisation = True):
    n = len(G.nodes)
    mcu = n**1.68-10
    subgraphs = ut.subgraph_one_edge_deletion(G)
    st = []
    for item in subgraphs:
        L = nx.laplacian_matrix(item).toarray()
        L = np.delete(L,0,axis=0)
        L = np.delete(L,0,axis=1)
        _,logdet = np.linalg.slogdet(L)
        det = math.exp(logdet)
        det = int(det)
        det = round(det,10)
        if det not in st:
            st.append(det)
    N1est = len(st)
    if normalisation == False:
        return N1est
    else:
        return (N1est-1)/(mcu-1)

#   Calculates C_{1e.spec} complexity of a graph
def C1espec(G,normalisation =True):
    subgraphs = ut.subgraph_one_edge_deletion(G)
    spectra = [];spectra_s = []
    mcu = len(G.nodes())**1.68-10
    for i in range(len(subgraphs)):
        L = nx.laplacian_matrix(subgraphs[i]).todense()
        A = nx.adjacency_matrix(subgraphs[i]).todense()
        eig_values,_ = eig(L)
        eig_values = eig_values.real
        eig_values = sorted(eig_values)
        eig_values = [round(item,10) for item in eig_values]
        L = L + A + A
        eig_values_s,_ = eig(L)
        eig_values_s = eig_values_s.real
        eig_values_s = sorted(eig_values_s)
        eig_values_s = [round(item,10) for item in eig_values_s]
        if eig_values not in spectra and eig_values_s not in spectra_s:
            spectra.append(eig_values)
            spectra_s.append(eig_values_s)
    N1espec = len(spectra)
    if normalisation == False:
        return N1espec
    else:
        return (N1espec-1)/(mcu-1)

#   Calculates C_{2e,spec} complexity of a graph
def C2espec(G,normalisation=True):
    n= len(G.nodes)
    remove_edges = []
    for i in range(n):
        neighbours = list(G.neighbors(i))
        for item in neighbours:
            if item < i:
                remove_edges.append([i,item])
    products = []
    for i in range(len(remove_edges)):
        for j in range(i+1,len(remove_edges)):
            products.append([remove_edges[i],remove_edges[j]])
    subgraphs = []
    for item in products:
        temp_G = G.copy()
        temp_G.remove_edge(item[0][0],item[0][1])
        temp_G.remove_edge(item[1][0],item[1][1])
        subgraphs.append(temp_G)
    spectra = [];spectra_s = []
    mcu = len(G.nodes())**1.68-10
    for i in range(len(subgraphs)):
        L = nx.laplacian_matrix(subgraphs[i]).todense()
        A = nx.adjacency_matrix(subgraphs[i]).todense()
        eig_values,_ = eig(L)
        eig_values = eig_values.real
        eig_values = sorted(eig_values)
        eig_values = [round(item,10) for item in eig_values]
        L = L + A + A
        eig_values_s,_ = eig(L)
        eig_values_s = eig_values_s.real
        eig_values_s = sorted(eig_values_s)
        eig_values_s = [round(item,10) for item in eig_values_s]
        if eig_values not in spectra and eig_values_s not in spectra_s:
            spectra.append(eig_values)
            spectra_s.append(eig_values_s)
    N2espec = len(spectra)
    if normalisation == False:
        return N2espec
    else:
        C2espec = (N2espec - 1)/(math.comb(int(mcu),2))
        return C2espec

#   Calculates MA_{g} of a graph
#   Using function mutual_info and redundancy from utilities to calcualte I and R
def MAg(G,normalisation = True):
    n = len(G.nodes)
    R = ut.redundancy(G)
    I = ut.mutual_info(G)
    R_p = 2*(n-2)/(n-1)*log(2)
    R_c = 2*log(n-1)
    I_p = log(n-1)-((n-3)/(n-1))*log(2)
    I_c = log((n)/(n-1))
    MAr = 4*((R-R_p)/(R_c - R_p))*(1 - (R-R_p)/(R_c-R_p))
    MAi = 4*((I-I_c)/(I_p-I_c))*(1-(I-I_c)/(I_p-I_c))
    if normalisation == True:
        return MAr * MAi
    else:
        return R*I
        
#   Calcualtes MA_{RI} complexity of a graph
#   Using function mutual_info and redundancy from utilities to calcualte I and R
def MAri(G,normalisation=True):
    n = len(G.nodes)
    R = ut.redundancy(G)
    I = ut.mutual_info(G)
    R_p = 2*log(2)*(n-2)/(n-1)
    R_c = 2*log(n-1)
    I_p = log(n-1)-log(2)*(n-3)/(n-1)
    I_c = log(n/(n-1))
    m=len(G.edges)
    numerator_1 = (R-R_p)
    numerator_2 = (I-I_c)
    denominator = 0.25*(log(2*m)-R_p-I_c)**2
    if normalisation == True:
        return numerator_1*numerator_2/denominator
    else:
        return R*I
\end{lstlisting}

\section{figure\_generator.py}
\begin{lstlisting}[breaklines=true,language=Python]
import utilities as ut
import networkx as nx
import Complexity as cx
import matplotlib.pyplot as plt
import collections
from math import log
import numpy as np
import pandas as pd
\end{lstlisting}
\label{figure_generator}
\# The following codes generate figure \ref{fig:path_clique}:
\begin{lstlisting}[breaklines=true,language=Python]
n = 10
G1 = nx.path_graph(n)
plt.figure(figsize=(10,5))
plt.subplot(1,2,1)
nx.draw(G1)
G2=nx.gnp_random_graph(n,1)
plt.subplot(1,2,2)
nx.draw(G2)
plt.show()
plt.savefig("figures/path_clique.eps",format="eps")
\end{lstlisting}
\# The following codes generate figure \ref{fig:small_graphs}:
\begin{lstlisting}[breaklines=true,language=Python]
import networkx as nx
import utilities as ut
import Complexity as cx
import matplotlib.pyplot as plt
import utilities as ut
import numpy as np
import matplotlib as mpl
from math import log
import pandas as pd
import collections
\end{lstlisting}
\noindent
\newline
\# Following codes generate figure \ref{fig:subgraph_measure}:
\begin{lstlisting}[breaklines=true,language=Python]
n=15
graphs,df = ut.random_networks(n,False,1000)

c1est = [cx.C1est(g) for g in graphs]
c1espec = [cx.C1espec(g) for g in graphs]
c2espec = [cx.C2espec(g) for g in graphs]

fig,axes = plt.subplots(1, 3,figsize = ([15,5]))
axes[0].scatter(df["Number_of_edges"],c1est,s=10,color = "black")
axes[1].scatter(df["Number_of_edges"],c1espec,s=10,color = "black")
axes[2].scatter(df["Number_of_edges"],c2espec,s=10,color = "black")
for item in axes:
    item.set_yticks([0,0.5,1,1.5])
    item.set_xlabel("m",fontsize =18)
axes[0].set_ylabel("$C1e,st$",fontsize= 18)
axes[1].set_ylabel("$C1e,spec$",fontsize= 18)
axes[2].set_ylabel("$C2e,spec$",fontsize= 18)
plt.show()
fig.savefig("figures/subgraph_measures.eps",format="eps")
\end{lstlisting}
\noindent
\newline
\# The following codes generate figure \ref{fig:c2espec}:
\begin{lstlisting}[breaklines=true,language=Python]
n = [6,7,8]
c2graphs= [];dfs= []
for item in n:
    g,d = ut.random_networks(item,True,50)
    c2graphs.append(g);dfs.append(d)
c2especs = []
for item in c2graphs:
    temp_result = [cx.C2espec(g) for g in item]
    c2especs.append(temp_result)
fig,axes = plt.subplots(1,3,figsize = (15,5))
for i in range(len(n)):
    axes[i].scatter(dfs[i]["Number_of_edges"],c2especs[i],s=10,color = "black")
    axes[i].set_yticks([0,0.5,1])
    axes[i].set_title("n="+str(n[i]),fontsize= 18)
    axes[i].set_xlabel("m",fontsize= 18)
axes[0].set_ylabel("$C_{1e,st}$",fontsize= 18)
axes[1].set_ylabel("$C_{1e,spec}$",fontsize= 18)
axes[2].set_ylabel("$C_{2e,spec}$",fontsize= 18)
plt.show()
fig.savefig("figures/c2espec.eps",format="eps")
\end{lstlisting}
\noindent
\newline
\# The following codes generate figure \ref{fig:small_world_models}:
\begin{lstlisting}[breaklines=true,language=Python]
plt.figure(figsize=(15,10))
plt.subplot(2,3,1)
nx.draw_circular(nx.watts_strogatz_graph(20,4,0),node_size=100)
plt.title("p = 0",fontsize= 18)
plt.subplot(2,3,2)
nx.draw_circular(nx.watts_strogatz_graph(20,4,0.1),node_size=100)
plt.title("p = 0.1",fontsize= 18)
plt.subplot(2,3,3)
nx.draw_circular(nx.watts_strogatz_graph(20,4,0.5),node_size=100)
plt.title("p = 0.5",fontsize= 18)
plt.subplot(2,3,4)
nx.draw_circular(nx.newman_watts_strogatz_graph(20,4,0),node_size=100)
plt.title("p = 0",fontsize= 18)
plt.subplot(2,3,5)
nx.draw_circular(nx.newman_watts_strogatz_graph(20,4,0.1),node_size=100)
plt.title("p = 0.1",fontsize= 18)
plt.subplot(2,3,6)
nx.draw_circular(nx.newman_watts_strogatz_graph(20,4,0.5),node_size=100)
plt.title("p = 0.5",fontsize= 18)
plt.show()
plt.savefig("figures/small_world_network_model.eps",format = "eps")
\end{lstlisting}
\noindent
\newline
\# Following codes generate figure \ref{fig:degree_dist}:
\begin{lstlisting}[breaklines=true,language=Python]
def degree_distribution(G):
    degree_sequence = sorted([d for n, d in G.degree()],reverse = True)
    degreeCount = collections.Counter(degree_sequence)
    deg, cnt = zip(*degreeCount.items())
    deg = list(deg)
    cnt = list(cnt)
    return deg,cnt
plt.figure(figsize=(15,6))
plt.subplot(1,2,1)
G = nx.gnm_random_graph(1000,5000)
deg,cnt = degree_distribution(G)
cnt = [item/1000 for item in cnt]
plt.plot(deg,cnt,color = "black",label="$G(n,m)$ model")
G = nx.barabasi_albert_graph(1000,5)
deg1,cnt1 = degree_distribution(G,)
cnt1 = [item/1000 for item in cnt1]
plt.plot(deg1,cnt1,'--',color = "black",label = "Barabasi-Albert model")
plt.legend(fontsize=12)
plt.scatter(deg,cnt,color = "black",marker = "x")
plt.scatter(deg1,cnt1,color = "black")
plt.xlabel("Degree",fontsize= 18)
plt.ylabel("Probability",fontsize= 18)
plt.title("Degree distribution",fontsize= 15)
plt.subplot(1,2,2)
log_deg = [log(item) for item in deg];log_cnt = [log(item) for item in cnt]
log_deg1 = [log(item) for item in deg1];log_cnt1 = [log(item) for item in cnt1]
plt.scatter(log_deg,log_cnt,marker = "x",color = "grey",s=15,label = "$G(n,m) model$")
plt.scatter(log_deg1,log_cnt1,marker = "o",color = "black",s=15,label = "Barabasi-Albert model")
plt.legend(fontsize=12)
plt.title("Log-log plot of the degree distribution")
plt.savefig("figures/degree_distribution.eps",format = "eps")
\end{lstlisting}
\noindent
\newline
\# The following codes generate figure \ref{fig:marimagcompare}:
\begin{lstlisting}[breaklines=true,language=Python]
n = 20
graphs,df = ut.random_networks(n,False,1000)
mari = [cx.MAri(g) for g in graphs]
mag = [cx.MAg(g) for g in graphs]
plt.figure()
plt.scatter(df["Number_of_edges"],mari,label = "$MA_{RI}$",color = "black",marker = "o",s=10)
plt.scatter(df["Number_of_edges"],mag, label = "$MA_{g}$",color = "grey",marker = "x",s=10)
plt.legend(fontsize=12)
plt.xlabel("m",fontsize= 18)
plt.ylabel("Complexity",fontsize= 18)
plt.savefig("figures/mariandmag.eps",format="eps")
\end{lstlisting}
\begin{lstlisting}[breaklines=true,language=Python]
methods = ["C1est","C1espec","C2espec","OdC","MAg","MAri","Cr","Ce"]
#Generates random graphs and data
n=7
graphs,df = ut.random_networks(n=n,use_all_m = True,sample_number = 50)
#Find the complexities of the graphs
results = []
for item in methods:
    method = getattr(cx,item)
    temp_result = [method(g) for g in graphs]
    results.append(temp_result)
c=0
fig,axes = plt.subplots(4,2,figsize = (10,16))
xticks = np.linspace(n-1,n*(n-1)/2,5)
xticks = [int(item) for item in xticks]
for i in range(4):
    for j in range(2):
        axes[i][j].scatter(df["Number_of_edges"],results[c],s=10,color = "black")
        axes[i][j].set_yticks([0,0.5,1])
        axes[i][j].set_xticks(xticks)
        axes[i][j].set_xlabel("m",fontsize = 18)
        axes[i][j].set_ylabel("$"+methods[c]+"$",fontsize = 18)
        c+=1
axes[0][0].set_ylabel("$C_{1e,st}$",fontsize= 18)
axes[0][1].set_ylabel("$C_{1e,spec}$",fontsize= 18)
axes[1][0].set_ylabel("$C_{2e,spec}$",fontsize= 18)
plt.savefig("figures/complexities.eps",format="eps")
plt.show()
\end{lstlisting}
\noindent
\newline
\# Following codes generate figure \ref{fig:correlation}:
\begin{lstlisting}[breaklines=true,language=Python]
n=25
corr_use_graphs,df = ut.random_networks(n,False,1000)
corr_results = []
corr_results.append([cx.C1est(g) for g in corr_use_graphs])
corr_results.append([cx.OdC(g) for g in corr_use_graphs])
corr_results.append([cx.MAri(g) for g in corr_use_graphs])
corr_use_BA = ut.BA_random_graphs(n,200)
corr_results_BA = []
corr_results_BA.append([cx.C1est(g) for g in corr_use_BA])
corr_results_BA.append([cx.OdC(g) for g in corr_use_BA])
corr_results_BA.append([cx.MAri(g) for g in corr_use_BA])
plt.figure(figsize=(22,7))
plt.subplot(1,3,1)
plt.scatter(corr_results[0],
                corr_results[1],
                s=15,c = df["Number_of_edges"],cmap = "binary",label = "$G(n,m)$ random graphs")
plt.scatter(corr_results_BA[0],corr_results_BA[1],color="black",marker = "x",label = "BA Graphs")
plt.xlabel("$C_{1e,st}$",fontsize = 20);plt.ylabel("$OdC$",fontsize = 20);

plt.subplot(1,3,2)
plt.scatter(corr_results[0],
                corr_results[2],
                s=15,c = df["Number_of_edges"],cmap = "binary",label = "$G(n,m)$ random graphs")
plt.scatter(corr_results_BA[0],corr_results_BA[2],color="black",marker = "x",label = "BA Graphs")
plt.xlabel("$C_{1e,st}$",fontsize = 20);plt.ylabel("$MAri$",fontsize = 20);
plt.legend(fontsize = 15)
plt.subplot(1,3,3)
plt.scatter(corr_results[1],
                corr_results[2],
                s=15,c = df["Number_of_edges"],cmap = "binary",label = "$G(n,m)$ random graphs")
plt.colorbar()
plt.scatter(corr_results_BA[1],corr_results_BA[2],color="black",marker = "x",label = "BA Graphs")
plt.xlabel("$OdC$",fontsize = 20);plt.ylabel("$MAri$",fontsize = 20);
plt.show()
plt.savefig("figures/complexity_correlation.eps",format = "eps")
\end{lstlisting}
\noindent
\newline
\# Following codes generate figure \ref{fig:graph_models}:
\begin{lstlisting}[breaklines=true,language=Python]
#Generates special random graphs
n = 20
randoms,random_df = ut.random_networks(n,False,500)
BA_graphs = ut.BA_random_graphs(n=n,sample_number = 100)
WS_graphs = ut.WS_random_graphs(n=n,sample_number = 100)
NW_graphs = ut.NW_random_graphs(n=n,sample_number = 100)
#Calculates the complexity of special random graphs
rd_result = []
for item in methods:
    method = getattr(cx,item)
    temp_result = [method(g) for g in randoms]
    rd_result.append(temp_result)
BA_result = []
for item in methods:
    method = getattr(cx,item)
    temp_result = [method(g) for g in BA_graphs]
    BA_result.append(temp_result)
WS_result = []
for item in methods:
    method = getattr(cx,item)
    temp_result = [method(g) for g in WS_graphs]
    WS_result.append(temp_result)
NW_result = []
for item in methods:
    method = getattr(cx,item)
    temp_result = [method(g) for g in NW_graphs]
    NW_result.append(temp_result)
# Calculates the complexities of special graphs
n=20
c=0
fig,axes = plt.subplots(4,2,figsize = (10,16))
xticks = np.linspace(n-1,n*(n-1)/2,5)
xticks = [int(item) for item in xticks]
for i in range(4):
    for j in range(2):
        axes[i][j].scatter([len(g.edges) for g in randoms],rd_result[c],s=15,color = "black",alpha = 0.7,label = "$G(n,m)$")
        axes[i][j].scatter([len(g.edges) for g in BA_graphs],BA_result[c],s=15,color = "green",label = "BA")
        axes[i][j].scatter([len(g.edges) for g in WS_graphs],WS_result[c],marker = "x",s=15,color = "red",label = "WS")
        axes[i][j].scatter([len(g.edges) for g in NW_graphs],NW_result[c],marker = "s",s=15,color = "blue",label = "NW")
        axes[i][j].set_yticks([0,0.5,1])
        axes[i][j].set_ylabel("$"+methods[c]+"$",fontsize = 20)
        axes[i][j].set_xlabel("m",fontsize = 20)
        axes[i][j].set_xticks(xticks)
        c+=1
axes[0][0].set_ylabel("$C_{1e,st}$")
axes[0][1].set_ylabel("$C_{1e,spec}$")
axes[1][0].set_ylabel("$C_{2e,spec}$")
axes[0][0].legend()
plt.legend(fontsize = 12)
plt.savefig("figures/complexities_sp.eps",format="eps")
plt.show()
\end{lstlisting}
\noindent
\newline
\# The following codes generate figure \ref{fig:complement}:
\begin{lstlisting}[breaklines=true,language=Python]
#Analyse Complement graphs complexity
graphs,df = ut.random_networks(25,False,1000)
BA_graphs = ut.BA_random_graphs(25, 100)
complement_graphs = [ut.complement_graph(g) for g in graphs]
c = []
g = []
edges = []
for i in range(len(complement_graphs)):
    if complement_graphs[i] !=None:
        c.append(complement_graphs[i])
        g.append(graphs[i])
        edges.append(df["Number_of_edges"][i])
BA_comp = [ut.complement_graph(g) for g in BA_graphs]
ba_c = []
ba_g = []
for i in range(len(BA_comp)):
    if BA_comp[i] !=None:
        ba_c.append(BA_comp[i])
        ba_g.append(BA_graphs[i])
results1 = [cx.OdC(item) for item in g]
results_c = [cx.OdC(g) for g in c]
ba_result = [cx.OdC(g) for g in ba_c]
ba_result_c = [cx.OdC(g) for g in ba_g]
results2 = [cx.MAri(item) for item in g]
results_c_1 = [cx.MAri(g) for g in c]
ba_result_1 = [cx.MAri(g) for g in ba_c]
ba_result_c_1 = [cx.MAri(g) for g in ba_g]
results3 = [cx.C1est(item) for item in g]
results_c_2 = [cx.C1est(g) for g in c]
ba_result_2 = [cx.C1est(g) for g in ba_c]
ba_result_c_2 = [cx.C1est(g) for g in ba_g]
plt.figure(figsize = (18,5))
plt.subplot(1,3,1)
plt.scatter(results1,results_c,c = edges,cmap="gray",label = "$G(n,m)$ random graphs")
plt.colorbar()
plt.scatter(ba_result,ba_result_c,marker = "x",label = "BA graphs")
plt.xlabel("$OdC$ of original graph",fontsize = 20)
plt.ylabel("$OdC$ of complement graph",fontsize = 20)
plt.xticks([0,0.5,1])
plt.yticks([0,0.5,1])
plt.legend(fontsize = 12)
plt.subplot(1,3,2)
plt.scatter(results2,results_c_1,c = edges,cmap="gray",label = "$G(n,m)$ random graphs")
plt.colorbar()
plt.scatter(ba_result_1,ba_result_c_1,marker = "x",label = "BA graphs")
plt.xlabel("$MAri$ of original graph",fontsize = 20)
plt.ylabel("$MAri$ of complement graph",fontsize = 20)
plt.xticks([0,0.5,1])
plt.yticks([0,0.5,1])
plt.subplot(1,3,3)
plt.scatter(results3,results_c_2,c = edges,cmap="gray",label = "$G(n,m)$ random graphs")
plt.colorbar()
plt.scatter(ba_result_2,ba_result_c_2,marker = "x",label = "BA graphs")
plt.xlabel("$C_{1e,st}$ of original graph",fontsize = 20)
plt.ylabel("$C_{1e,st}$ of complement graph",fontsize = 20)
plt.xticks([0,0.5,1])
plt.yticks([0,0.5,1])
plt.legend(fontsize = 12)
plt.savefig("figures/complement.eps",format="eps")
\end{lstlisting}
\noindent
\newline
\# Following codes generate figure \ref{fig:real_networks}. Used graphs' descriptions can be found in table \ref{tb:descriptions}:
\begin{lstlisting}[breaklines=true,language=Python]
index = ["dolphins","pdzbase","hamsterster","Roget","flight","GBPT_train"]
index_bus = ["london","paris","berlin","sydney","detroit","beijing"]
load_path = ["real_networks/processed/"+item+".csv" for item in index]
load_path_bus = ["bus/processed/"+item+".csv" for item in index_bus]
load_path_bus_m = ["bus/modified/m_"+item+".csv" for item in index_bus]
graphs = [ut.df_to_network(pd.read_csv(item)) for item in load_path]
bus = [ut.df_to_network(pd.read_csv(item)) for item in load_path_bus]
bus_m = [ut.df_to_network(pd.read_csv(item)) for item in load_path_bus_m]
l = [nx.average_shortest_path_length(g) for g in graphs]       
l_bus = [nx.average_shortest_path_length(g) for g in bus]
l_bus_m = [nx.average_shortest_path_length(g) for g in bus_m]
lr = [ut.lr(g) for g in graphs]
lr_bus = [ut.lr(g) for g in bus]
lr_bus_m = [ut.lr(g) for g in bus_m]
result_g = [cx.MAri(g) for g in graphs]
result_bus = [cx.MAri(g) for g in bus]
result_bus_m = [cx.MAri(g) for g in bus_m]
generated_graphs = [
    nx.gnm_random_graph(875,4000),
    nx.watts_strogatz_graph(875,10,0.05),
    nx.newman_watts_strogatz_graph(875,9,0.05),
    nx.barabasi_albert_graph(1000,4)
]
l_gg_ratio =[]
for i in range(len(generated_graphs)):
    l_gg_ratio.append(nx.average_shortest_path_length(generated_graphs[i])/ut.lr(generated_graphs[i]))
gg_result = [cx.MAri(item) for item in generated_graphs]
plt.figure(figsize=(16,8))
l_ratio = [item0/item1 for item0,item1 in zip(l,lr)]
l_ratio_bus = [item0/item1 for item0,item1 in zip(l_bus,lr_bus)]
l_ratio_bus_m = [item0/item1 for item0,item1 in zip(l_bus_m,lr_bus_m)]
plt.scatter(l_ratio,result_g,color = "black",s=50,label = "Real networks")
plt.scatter(l_ratio_bus,result_bus,color = "red",s=50,marker = "x",label ="Bus networks")
plt.scatter(l_ratio_bus_m,result_bus_m,color = "blue",marker = "^",s=50,label = "Modified bus networks")
plt.scatter(l_gg_ratio,gg_result,color = "green",marker = "s",s=50,label = "Generated graphs")
plt.legend(fontsize = 15)
plt.xlabel("$L/L_r$",fontsize = 20)
plt.ylabel("$MAri$",fontsize = 20)
plt.yticks([0,0.25,0.5,0.75,1])
for i in range(len(index)):
    if i !=1:
        plt.annotate(str(i+1),(l_ratio[i]+0.02,result_g[i]+0.02),fontsize= 15,color = "grey")
for i in range(len(index)):
    plt.annotate(str(i+7),(l_ratio_bus[i]+0.02,result_bus[i]+0.02),fontsize= 15,color = "grey")
for i in range(len(index)):
    plt.annotate(str(i+13),(l_ratio_bus_m[i]+0.02,result_bus_m[i]+0.02),fontsize= 15,color = "grey")
for i in range(len(generated_graphs)):
    plt.annotate(str(i+19),(l_gg_ratio[i]+0.02,gg_result[i]+0.02),fontsize= 15,color = "grey")
plt.annotate("2",(1,0.275),fontsize=15,color="grey")
plt.savefig("figures/real_networks.eps",format = "eps")
\end{lstlisting}

\section{rewiring\_simulation.py}
This file is used to simulate both rewirings introduced in section \ref{rewiring}, and generate figure \ref{fig:rewiring}:
\begin{lstlisting}[breaklines=true,language=Python]
import utilities as ut
import Complexity as cx
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
class rewiring():
    def __init__(self,name,bus=False,step_prob = 0.05):
        self.name = name
        self.bus = bus
        self.load_network()
        self.step_prob = step_prob
        self.rewired_G = self.G.copy()
        self.result = []
        self.s_n=0
        self.p_n=0
        self.load_result("S")
        self.load_result("P")
    #Load all the networks
    def load_network(self):
        if self.bus == False:
            load_path = "real_networks/processed/"+self.name+".csv"
        else:
            load_path = "bus/modified/m_"+self.name+".csv"
        df = pd.read_csv(load_path)
        self.G = ut.df_to_network(df)
    #Carry out one step of single link rewiring
    def single_link_rewiring(self):
        self.rewired_G = ut.single_link_rewiring(self.rewired_G,self.step_prob)
    #Carry out one step of pairwise rewiring
    def pairwise_rewiring(self):
        self.rewired_G = ut.pairwise_rewiring(self.rewired_G,self.step_prob)
    #Reinstantiate the rewired graph
    def clear_rewiring(self):
        self.rewired_G = self.G.copy()
        if cx.OdC(self.rewired_G) != cx.OdC(self.G):
            Exception("Clearing failed")
    #Carry out a sample number of rewiring
    def continuous_rewiring(self,method):
        self.result = [cx.OdC(self.G)]
        if method == "S":
            niter = int(1/self.step_prob)
            for i in range(niter):
                self.single_link_rewiring()
                self.result.append(cx.OdC(self.rewired_G))
            self.s_n += 1
        elif method == "P":
            niter = int(1/self.step_prob)
            for i in range(niter):
                self.pairwise_rewiring()
                self.result.append(cx.OdC(self.rewired_G))
            self.p_n += 1
        else:
            raise ValueError('Insufficient method')
    #Load the calculated result from the data folder
    def load_result(self,method):
        if method =="S":
            try:
                self.s_result = pd.read_csv("result/single_link/"+self.name+".csv")
                self.s_n = len(self.s_result.columns)
            except FileNotFoundError:
                self.s_result = pd.DataFrame(data = {"1":[]})
                self.s_n = 0
        if method =="P":
            try:
                self.p_result = pd.read_csv("result/pairwise/"+self.name+".csv")
                self.p_n = len(self.p_result.columns)
            except FileNotFoundError:
                self.p_result = pd.DataFrame(data = {"1":[]})
                self.p_n = 0
    #Store the result in the class variable result
    def record_result(self,method):
        if method == "S":
            self.s_result[str(self.s_n)] = self.result
        elif method == "P":
            self.p_result[str(self.p_n)] = self.result
        else:
            raise ValueError('Insufficient method')
    #Save the current result table as a csv file
    def write_result(self,method):
        if method == "S" and self.s_n == 0:
            raise Exception("There is no recorded result")
        if method == "P" and self.p_n == 0:
            raise Exception("There is no recorded result")
        if (method == "S+P" or method == "P+S") and (self.s_n == 0 or self.p_n==0):
            raise Exception("There is no recorded result for one/both of the methods.")
        
        if method == "S":
            self.s_result.to_csv("result/single_link/"+self.name+".csv",index = False)
        elif method == "P":
            self.p_result.to_csv("result/pairwise/"+self.name+".csv",index = False)
        elif method == "S+P" or method == "P+S":
            self.s_result.to_csv("result/single_link/"+self.name+".csv",index = False)
            self.p_result.to_csv("result/pairwise/"+self.name+".csv",index = False)
        else:
            raise ValueError('Insufficient method')
    #Carry multiple number of rewiring according to the given parameter
    def experiment(self,method,sample_number):
        if type(sample_number) != type(int(1)):
            ValueError("Insufficient number of samples, please input an integer.")        
        for i in range(sample_number):
            self.clear_rewiring()
            self.continuous_rewiring(method)            
            self.record_result(method)
    #Plot the results
    def plot_result(self,method):
        if method == "S":
            if self.s_n != 0:
                prob_list = np.linspace(0,1,int(1/(self.step_prob)+1))
                mean = [np.mean(row) for index, row in self.s_result.iterrows()]
                std = [0.5*np.std(row) for index, row in self.s_result.iterrows()]
                plt.errorbar(prob_list,mean,yerr = std,color = "black")
            else:
                raise Exception("There is no recorded result")
        elif method == "P":
            if self.p_n != 0:
                prob_list = np.linspace(0,1,int(1/(self.step_prob)+1))
                mean = [np.mean(row) for index, row in self.p_result.iterrows()]
                std = [0.5*np.std(row) for index, row in self.p_result.iterrows()]
                plt.errorbar(prob_list,mean,yerr = std,color = "black")
            else:
                raise Exception("There is no recorded result")
        elif method == "P+S" or method == "S+P":
            if self.s_n !=0 and self.p_n != 0:
                prob_list = np.linspace(0,1,int(1/(self.step_prob)+1))
                mean = [np.mean(row) for index, row in self.s_result.iterrows()]
                std = [0.5*np.std(row) for index, row in self.s_result.iterrows()]
                plt.errorbar(prob_list,mean,yerr = std,color = "black",label = "Single link rewiring")
                mean = [np.mean(row) for index, row in self.p_result.iterrows()]
                std = [0.5*np.std(row) for index, row in self.p_result.iterrows()]
                plt.errorbar(prob_list,mean,yerr = std,color = "blue",label = "Pairwise rewiring")
                plt.legend()
                plt.xlabel("Rewiring Probability")
                plt.ylabel("OdC Complexity")
                plt.title("Change of complexity after rewiring")
            else:
                raise Exception("There is no recorded result for one/both of the methods.")
        else:
            raise ValueError('Insufficient method')
    #Carry out a given number of rewiring, saving the result and plotting at the same time.
    def one_stop(self,method,sample_number):
        self.experiment(method,sample_number)
        self.write_result(method)
        self.plot_result(method)
index = ["dolphins","pdzbase","GBPT_train",
         "hamsterster","Roget","flight"]
index_bus = ["london","paris","berlin","sydney","detroit","beijing"]
real_networks = [rewiring(item) for item in index]
bus_networks = [rewiring(item,bus = True) for item in index_bus]
for niter in range(10):
    for item in real_networks:
        item.experiment("P",1)
        item.write_result("P")
        item.experiment("S",1)
        item.write_result("S")
    for item in bus_networks:
        item.experiment("P",1)
        item.write_result("P")
        item.experiment("S",1)
        item.write_result("S")
plt.figure(figsize=(20,20))
step_prob = 0.05
prob_list = np.linspace(0,1,int(1/(step_prob)+1))
for i in range(len(real_networks)):
    plt.subplot(4,3,i+1)
    mean = [np.mean(row) for index, row in real_networks[i].s_result.iterrows()]
    std = [np.std(row) for index, row in real_networks[i].s_result.iterrows()]
    plt.errorbar(prob_list,mean,yerr = std,color = "black",label = "Single link rewiring")
    mean = [np.mean(row) for index, row in real_networks[i].p_result.iterrows()]
    std = [np.std(row) for index, row in real_networks[i].p_result.iterrows()]
    plt.errorbar(prob_list,mean,yerr = std,color = "blue",label = "Pairwise rewiring")
    plt.title(index[i].capitalize(),fontsize=15)
for i in range(len(real_networks)):
    plt.subplot(4,3,6+i+1)
    mean = [np.mean(row) for index, row in bus_networks[i].s_result.iterrows()]
    std = [np.std(row) for index, row in bus_networks[i].s_result.iterrows()]
    plt.errorbar(prob_list,mean,yerr = std,color = "black",label = "Single link rewiring")
    mean = [np.mean(row) for index, row in bus_networks[i].p_result.iterrows()]
    std = [np.std(row) for index, row in bus_networks[i].p_result.iterrows()]
    plt.errorbar(prob_list,mean,yerr = std,color = "blue",label = "Pairwise rewiring")
    plt.title(index_bus[i].capitalize(),fontsize=15)
plt.subplot(4,3,1);plt.ylabel("$OdC$",fontsize=18);plt.legend(fontsize = 12);plt.title("Dolphins",fontsize=15)
plt.subplot(4,3,2);plt.title("PDZBase",fontsize=15)
plt.subplot(4,3,3);plt.title("UK train",fontsize=15)
plt.subplot(4,3,4);plt.ylabel("$OdC$",fontsize=18);
plt.subplot(4,3,5);plt.title("Roget's thesaurus",fontsize=15)
plt.subplot(4,3,7);plt.ylabel("$OdC$",fontsize=18)
plt.subplot(4,3,10);plt.xlabel("Rewiring probability",fontsize=18);plt.ylabel("$OdC$",fontsize=18)
plt.subplot(4,3,11);plt.xlabel("Rewiring probability",fontsize=18)
plt.subplot(4,3,12);plt.xlabel("Rewiring probability",fontsize=18);plt.legend(fontsize = 12)
plt.savefig("figures/rewiring.eps",format = "eps")
\end{lstlisting}
\section{configuration\_model.r and R\_functions.py}
Since no package can implement a connected configuration model in Python, R is used to implement the configuration model and simulate the behaviour to produce figure \ref{fig:configuration_model}.
\\
configuration\_model.r:
\begin{lstlisting}[breaklines=true,language=R]
#include necessary libraries, and setting up the environment
library("reticulate")
library("igraph")
library("plotly")
use_python("/user/python3")
#function1 return the edges list to build the network in networkx
#function2 return the graph itself for tespecing in the R environment
configuration_model <- function(series){
  g <- sample_degseq(series, method="vl")
  return(as_edgelist(g))
}
return_graph<- function(series){
  g <- sample_degseq(series, method="vl")
  return(g)
}
#python functions
source_python("R_functions.py")
#Building the network in python
n = 50
gamma = 3
gamma_list = seq(gamma-0.3,gamma+0.3,0.01)
mari_results = integer(length(gamma_list))
OdC_results = integer(length(gamma_list))
C1espec_results = integer(length(gamma_list))
graphs = list()
for(niter in 1:50){
for(i in 1:length(gamma_list)){
  series = power_law_series(n,gamma_list[i])
  if(sum(series)%%2 !=0){
    series[1]=series[1]+1
  }
  edge_list = configuration_model(series)
  g = construct_network(edge_list)
  graphs = append(graphs,g)
  mari_results[i] = mari_results[i]+MAri(g)
  OdC_results[i] = OdC_results[i]+OdC(g)
  C1espec_results[i] = C1espec_results[i]+C1espec(g)
}
  }
mari_results = mari_results/niter
OdC_results = OdC_results/niter
C1espec_results = C1espec_results/niter
dev.new(3,5)
plot(gamma_list, C1espec_results, col="black",pch="*","l", lty=1, ylim=c(0,0.5), ylab="Complexity",xlab="Gamma" )
lines(gamma_list, OdC_results, col="blue",lty=2)
lines(gamma_list, mari_results, col="red", lty=3)
legend(3.1,0.45,legend=c(expression(paste('C'["1e,spec"])),"OdC","MAri"),col=c("black","blue","red"),lty=c(1,2,3), ncol=1)
\end{lstlisting}
\noindent
\newline
R\_functions.py:
\begin{lstlisting}[breaklines=true,language=Python]
#Complexity measure of C_{1e,spec}, OdC and MA_{RI} can be found in Complexity.py, please copy and paste them here.
def ln(x):
    if x == 0:
        return 0
    else:
        return np.log(x)
def power_law_series(n,gamma):
    n = int(n)
    sequence = nx.random_powerlaw_tree_sequence(n, gamma,tries=100000)
    return sequence
def construct_network(edge_list):
    G=nx.Graph()
    for item in edge_list:
        G.add_edge(item[0],item[1])
    return G 
\end{lstlisting}

\printbibliography[heading=bibnumbered]

\end{document}
